{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuslanKozlyak/BackgroundRemover/blob/master/Watermark_removal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC2QwVwP5Qbx"
      },
      "source": [
        "# INIT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clhK49poGMIU"
      },
      "source": [
        "## PIP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNhiiAve23DA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6199b55c-4ad0-4b7b-afc1-d1d494b993f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.0/612.0 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U opencv-python-headless==4.5.4.60\n",
        "!pip install -q -U albumentations\n",
        "#!pip install -q -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "#!pip install -q -U torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q GPUtil\n",
        "!pip install -q -U --no-cache-dir gdown --pre\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q kornia\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uf5bLx3GSCM"
      },
      "source": [
        "## WANDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA93ZhgPUJQ3",
        "outputId": "4ccd9665-68e9-4634-e43f-45e817a8c039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login  ff1a14bc5c39ecec7892c71fea3b087bd64c7131\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBAdlgReGVeL"
      },
      "source": [
        "## DOWNLOAD DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siqLupTM3IOc"
      },
      "outputs": [],
      "source": [
        "! gdown -q 11K86aMWWy9jsStj9JlVpfWefkbdGd4ZD \n",
        "!unzip -q /content/WateramaksDatasetTest.zip -d  /content/\n",
        "\n",
        "! gdown -q 1JqF5EYffHpo1dU9nuh1uhnLGcXpbfSA_\n",
        "!unzip -q /content/Validation.zip -d  /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCv9h06kGbeQ"
      },
      "source": [
        "## IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqBv2cCef71F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from os import listdir\n",
        "import albumentations as A\n",
        "from torch import optim\n",
        "from functools import partial\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torch.nn import BatchNorm2d\n",
        "from torchvision import transforms\n",
        "from urllib.request import urlretrieve\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import utils as vutils\n",
        "from PIL import Image,ImageDraw, ImageFont\n",
        "from torchvision.transforms import transforms\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from typing import Tuple, Dict, Optional,List\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.nn.utils import weight_norm as weight_norm_fn\n",
        "from torch.nn.utils import spectral_norm as spectral_norm_fn\n",
        "from torchmetrics import PeakSignalNoiseRatio,StructuralSimilarityIndexMeasure\n",
        "from kornia.geometry.transform import rotate\n",
        "import sys, traceback, gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r3TRy875mO-"
      },
      "source": [
        "## DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbHezHdG5eZx"
      },
      "outputs": [],
      "source": [
        "class CarvanaDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.watermarks = []\n",
        "        self.watemarked_images = []\n",
        "\n",
        "        path = image_dir + 'images/'\n",
        "        for i in listdir(path):\n",
        "            self.images.append(os.path.join(path, i))\n",
        "\n",
        "        path = image_dir + 'watermarks/'\n",
        "        for i in listdir(path):\n",
        "            self.watermarks.append(os.path.join(path, i))\n",
        "\n",
        "        path = image_dir + 'watemarked_images/'\n",
        "        for i in listdir(path):\n",
        "            self.watemarked_images.append(os.path.join(path, i))\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
        "        mask = Image.open(self.watermarks[index]).convert(\"L\")\n",
        "        masked_img = Image.open(self.watemarked_images[index]).convert(\"RGB\")\n",
        "        width, height = img.size\n",
        "        image = np.array(img, dtype=float) / 255\n",
        "        watermark = np.array(mask, dtype=float) / 255\n",
        "        watemarked_image = np.array(masked_img, dtype=float) / 255\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image, mask=watermark)\n",
        "            t_image = augmentations[\"image\"]\n",
        "            t_mask = augmentations[\"mask\"]\n",
        "            augmentations = self.transform(image=watemarked_image)\n",
        "            t_masked_image = augmentations[\"image\"]\n",
        "   \n",
        "        return t_image.type(torch.FloatTensor), t_mask.unsqueeze(0).type(torch.FloatTensor),  t_masked_image.type(torch.FloatTensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Yx0O2PLU2x"
      },
      "source": [
        "## UTILS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx_pGeMa5tdy"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model,state, filename=\"checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "    model.add_file(filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def make_checkpoint(model,optimizer):\n",
        "    checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "    return checkpoint\n",
        "\n",
        "\n",
        "def get_loaders(\n",
        "        train_dir,\n",
        "        val_dir,\n",
        "        batch_size,\n",
        "        train_transform,\n",
        "        val_transform,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "):\n",
        "    train_ds = CarvanaDataset(\n",
        "        image_dir=train_dir,\n",
        "        transform=train_transform,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_ds = CarvanaDataset(\n",
        "        image_dir=val_dir,\n",
        "        transform=val_transform,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "\n",
        "def check_accuracy(loader,  gen, disc, gen_optimizer, disc_optimizer, loss_fn):\n",
        "    print(\"=> Checking accuracy\")\n",
        "    gen.eval()\n",
        "    loop = tqdm(loader)\n",
        "    psnr = PeakSignalNoiseRatio().to(device=DEVICE)\n",
        "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device=DEVICE)\n",
        "    sum_gen_loss = 0\n",
        "    sum_disc_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (image, mask, masked_image) in enumerate(loop):\n",
        "        masked_image = masked_image.to(device=DEVICE)\n",
        "        image = image.to(device=DEVICE)\n",
        "        mask = mask.to(device=DEVICE)\n",
        "        masked_img = torch.cat([masked_image, mask], dim=1)\n",
        "        masked_img = masked_img.to(device=DEVICE)\n",
        "        pred = gen(masked_img)\n",
        "        gen_loss = loss_fn(image, pred, mask, disc)\n",
        "\n",
        "        sum_gen_loss += gen_loss.item()\n",
        "        PSNR = psnr(pred,image).item()\n",
        "        SSIM = ssim(pred,image).item()\n",
        "        \n",
        "        loop.set_postfix(gen_loss=gen_loss.item(),                       \n",
        "                         PSNR=PSNR,\n",
        "                         SSIM=SSIM)\n",
        "        if batch_idx == 0:\n",
        "          log_image_table(masked_image, pred)\n",
        "\n",
        "    PSNR = psnr.compute().item()\n",
        "    ssim = ssim.compute().item()\n",
        "    wandb.log({\"PSNR_mean\": PSNR, \"SSIM_mean\": SSIM,\"Loss_mean\":sum_gen_loss/len(loop)})     \n",
        "    gen.train()\n",
        "\n",
        "\n",
        "def log_image_table(images, predicted):\n",
        "    table = wandb.Table(columns=[\"image\", \"pred\"])\n",
        "    for img, pred in zip(images.to(\"cpu\"), predicted.to(\"cpu\")):\n",
        "        table.add_data(wandb.Image(img[0].numpy()*255), wandb.Image(pred[0].numpy()*255))\n",
        "    wandb.log({\"predictions_table\":table}, commit=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7-ciyV15tcn"
      },
      "source": [
        "## SETTINGS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tVGZzjR6XR_"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"gen_rate\": 0.001,\n",
        "    \"disc_rate\": 0.0001,\n",
        "    \"architecture\": \"LAMA\",\n",
        "    \"dataset\": \"COCO\",\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 8,\n",
        "    }\n",
        "\n",
        "LEARNING_RATE = config['learning_rate']\n",
        "GEN_LEARNING_RATE = config['gen_rate']\n",
        "DISC_LEARNING_RATE = config['disc_rate']\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = config['batch_size']\n",
        "NUM_EPOCHS = config['epochs']\n",
        "W = 128\n",
        "H = 128\n",
        "NUM_WORKERS = 1\n",
        "\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = True\n",
        "VAL_DIR = r\"/content/Validation/\"\n",
        "TRAIN_DIR = r\"/content/WateramaksDatasetTest/\"\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(W, H, p=1),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        ")\n",
        "\n",
        "val_transforms = A.Compose(\n",
        "    [\n",
        "        A.Resize(W, H, p=1),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        ")\n",
        "\n",
        "train_loader, val_loader = get_loaders(\n",
        "    TRAIN_DIR,\n",
        "    VAL_DIR,\n",
        "    BATCH_SIZE,\n",
        "    train_transform,\n",
        "    val_transforms,\n",
        "    NUM_WORKERS,\n",
        "    PIN_MEMORY,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWi7L_i19_Sl"
      },
      "source": [
        "# LAMA 🦙\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIVZxHbKNFrs"
      },
      "source": [
        "## SETTINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL69Uk8aNJkZ"
      },
      "outputs": [],
      "source": [
        "IN_CHANNELS = 4\n",
        "n_downsampling = 3\n",
        "n_blocks = 9\n",
        "\n",
        "init_conv_kwargs = {\n",
        "    \"ratio_gin\": 0,\n",
        "  \"ratio_gout\": 0,\n",
        "  \"enable_lfu\": False\n",
        "}\n",
        "\n",
        "downsample_conv_kwargs = {\n",
        "      \"ratio_gin\": init_conv_kwargs [\"ratio_gout\"],\n",
        "  \"ratio_gout\": init_conv_kwargs[\"ratio_gout\"],\n",
        "  \"enable_lfu\": False\n",
        "\n",
        "}\n",
        "\n",
        "resnet_conv_kwargs = {\n",
        "     \"ratio_gin\": 0.75,\n",
        "  \"ratio_gout\": 0.75,\n",
        "  \"enable_lfu\": False\n",
        "}\n",
        "\n",
        "conv_kwargs = {\n",
        "         \"ratio_gin\": 0,\n",
        "  \"ratio_gout\": 0,\n",
        "  \"enable_lfu\": False\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-xlBAUW--jh"
      },
      "source": [
        "## BASE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gpw9sU0BJt5",
        "outputId": "2ede1fbf-1e53-4a10-9f98-490c5f5e272c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all ok\n"
          ]
        }
      ],
      "source": [
        "class LearnableSpatialTransformWrapper(nn.Module):\n",
        "    def __init__(self, impl, pad_coef=0.5, angle_init_range=80):\n",
        "        super().__init__()\n",
        "        self.impl = impl\n",
        "        self.angle = torch.rand(1) * angle_init_range\n",
        "        self.angle = nn.Parameter(self.angle, requires_grad=True)\n",
        "        self.pad_coef = pad_coef\n",
        "\n",
        "    def forward(self, x):\n",
        "        if torch.is_tensor(x):\n",
        "            return self.inverse_transform(self.impl(self.transform(x)), x)\n",
        "        else:\n",
        "            raise ValueError(f'Unexpected input type {type(x)}')\n",
        "\n",
        "    def transform(self, x):\n",
        "        height, width = x.shape[2:]\n",
        "        pad_h, pad_w = int(height * self.pad_coef), int(width * self.pad_coef)\n",
        "        x_padded = F.pad(x, [pad_w, pad_w, pad_h, pad_h], mode='reflect')\n",
        "        x_padded_rotated = rotate(x_padded, angle=self.angle.to(x_padded))\n",
        "        return x_padded_rotated\n",
        "\n",
        "    def inverse_transform(self, y_padded_rotated, orig_x):\n",
        "        height, width = orig_x.shape[2:]\n",
        "        pad_h, pad_w = int(height * self.pad_coef), int(width * self.pad_coef)\n",
        "\n",
        "        y_padded = rotate(y_padded_rotated, angle=-self.angle.to(y_padded_rotated))\n",
        "        y_height, y_width = y_padded.shape[2:]\n",
        "        y = y_padded[:, :, pad_h : y_height - pad_h, pad_w : y_width - pad_w]\n",
        "        return y\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    layer = LearnableSpatialTransformWrapper(nn.Identity())\n",
        "    x = torch.arange(2* 3 * 15 * 15).view(2, 3, 15, 15).float()\n",
        "    y = layer(x)\n",
        "    assert x.shape == y.shape\n",
        "    print('all ok')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mdZ_Xr9-LGh"
      },
      "source": [
        "## FFC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kszPtaKv-N23"
      },
      "outputs": [],
      "source": [
        "class FourierUnit(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, groups=1, spatial_scale_factor=None, spatial_scale_mode='bilinear',\n",
        "                 ffc3d=False, fft_norm='ortho'):\n",
        "        # bn_layer not used\n",
        "        super(FourierUnit, self).__init__()\n",
        "        self.groups = groups\n",
        "        self.conv_layer = torch.nn.Conv2d(in_channels=in_channels * 2,\n",
        "                                          out_channels=out_channels * 2,\n",
        "                                          kernel_size=1, stride=1, padding=0, groups=self.groups, bias=False)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels * 2)\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "        self.spatial_scale_factor = spatial_scale_factor\n",
        "        self.spatial_scale_mode = spatial_scale_mode\n",
        "        self.ffc3d = ffc3d\n",
        "        self.fft_norm = fft_norm\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch = x.shape[0]\n",
        "\n",
        "        if self.spatial_scale_factor is not None:\n",
        "            orig_size = x.shape[-2:]\n",
        "            x = F.interpolate(x, scale_factor=self.spatial_scale_factor, mode=self.spatial_scale_mode, align_corners=False)\n",
        "        r_size = x.size()\n",
        "        # (batch, c, h, w/2+1, 2)\n",
        "\n",
        "        fft_dim = (-3, -2, -1) if self.ffc3d else (-2, -1)\n",
        "        ffted = torch.fft.rfftn(x, dim=fft_dim, norm=self.fft_norm)\n",
        "        ffted = torch.stack((ffted.real, ffted.imag), dim=-1)\n",
        "        ffted = ffted.permute(0, 1, 4, 2, 3).contiguous()  # (batch, c, 2, h, w/2+1)\n",
        "        ffted = ffted.view((batch, -1,) + ffted.size()[3:])\n",
        "\n",
        "        ffted = self.conv_layer(ffted)  # (batch, c*2, h, w/2+1)\n",
        "        ffted = self.relu(self.bn(ffted))\n",
        "\n",
        "        ffted = ffted.view((batch, -1, 2,) + ffted.size()[2:]).permute(\n",
        "            0, 1, 3, 4, 2).contiguous()  # (batch,c, t, h, w/2+1, 2)\n",
        "        ffted = torch.complex(ffted[..., 0], ffted[..., 1])\n",
        "\n",
        "        ifft_shape_slice = x.shape[-3:] if self.ffc3d else x.shape[-2:]\n",
        "        output = torch.fft.irfftn(ffted, s=ifft_shape_slice, dim=fft_dim, norm=self.fft_norm)\n",
        "\n",
        "        if self.spatial_scale_factor is not None:\n",
        "            output = F.interpolate(output, size=orig_size, mode=self.spatial_scale_mode, align_corners=False)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class SpectralTransform(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, groups=1, enable_lfu=True, **fu_kwargs):\n",
        "        # bn_layer not used\n",
        "        super(SpectralTransform, self).__init__()\n",
        "        self.enable_lfu = enable_lfu\n",
        "        if stride == 2:\n",
        "            self.downsample = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n",
        "        else:\n",
        "            self.downsample = nn.Identity()\n",
        "\n",
        "        self.stride = stride\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels //\n",
        "                      2, kernel_size=1, groups=groups, bias=False),\n",
        "            nn.BatchNorm2d(out_channels // 2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.fu = FourierUnit(\n",
        "            out_channels // 2, out_channels // 2, groups, **fu_kwargs)\n",
        "        if self.enable_lfu:\n",
        "            self.lfu = FourierUnit(\n",
        "                out_channels // 2, out_channels // 2, groups)\n",
        "        self.conv2 = torch.nn.Conv2d(\n",
        "            out_channels // 2, out_channels, kernel_size=1, groups=groups, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.downsample(x)\n",
        "        x = self.conv1(x)\n",
        "        output = self.fu(x)\n",
        "\n",
        "        if self.enable_lfu:\n",
        "            n, c, h, w = x.shape\n",
        "            split_no = 2\n",
        "            split_s = h // split_no\n",
        "            xs = torch.cat(torch.split(\n",
        "                x[:, :c // 4], split_s, dim=-2), dim=1).contiguous()\n",
        "            xs = torch.cat(torch.split(xs, split_s, dim=-1),\n",
        "                           dim=1).contiguous()\n",
        "            xs = self.lfu(xs)\n",
        "            xs = xs.repeat(1, 1, split_no, split_no).contiguous()\n",
        "        else:\n",
        "            xs = 0\n",
        "\n",
        "        output = self.conv2(x + output + xs)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class FFC(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 ratio_gin, ratio_gout, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=False, enable_lfu=True,\n",
        "                 padding_type='reflect', gated=False, **spectral_kwargs):\n",
        "        super(FFC, self).__init__()\n",
        "\n",
        "        assert stride == 1 or stride == 2, \"Stride should be 1 or 2.\"\n",
        "        self.stride = stride\n",
        "\n",
        "        in_cg = int(in_channels * ratio_gin)\n",
        "        in_cl = in_channels - in_cg\n",
        "        out_cg = int(out_channels * ratio_gout)\n",
        "        out_cl = out_channels - out_cg\n",
        "        #groups_g = 1 if groups == 1 else int(groups * ratio_gout)\n",
        "        #groups_l = 1 if groups == 1 else groups - groups_g\n",
        "\n",
        "        self.ratio_gin = ratio_gin\n",
        "        self.ratio_gout = ratio_gout\n",
        "        self.global_in_num = in_cg\n",
        "\n",
        "        module = nn.Identity if in_cl == 0 or out_cl == 0 else nn.Conv2d\n",
        "        self.convl2l = module(in_cl, out_cl, kernel_size,\n",
        "                              stride, padding, dilation, groups, bias, padding_mode=padding_type)\n",
        "        module = nn.Identity if in_cl == 0 or out_cg == 0 else nn.Conv2d\n",
        "        self.convl2g = module(in_cl, out_cg, kernel_size,\n",
        "                              stride, padding, dilation, groups, bias, padding_mode=padding_type)\n",
        "        module = nn.Identity if in_cg == 0 or out_cl == 0 else nn.Conv2d\n",
        "        self.convg2l = module(in_cg, out_cl, kernel_size,\n",
        "                              stride, padding, dilation, groups, bias, padding_mode=padding_type)\n",
        "        module = nn.Identity if in_cg == 0 or out_cg == 0 else SpectralTransform\n",
        "        self.convg2g = module(\n",
        "            in_cg, out_cg, stride, 1 if groups == 1 else groups // 2, enable_lfu, **spectral_kwargs)\n",
        "\n",
        "        self.gated = gated\n",
        "        module = nn.Identity if in_cg == 0 or out_cl == 0 or not self.gated else nn.Conv2d\n",
        "        self.gate = module(in_channels, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_l, x_g = x if type(x) is tuple else (x, 0)\n",
        "        out_xl, out_xg = 0, 0\n",
        "\n",
        "        if self.gated:\n",
        "            total_input_parts = [x_l]\n",
        "            if torch.is_tensor(x_g):\n",
        "                total_input_parts.append(x_g)\n",
        "            total_input = torch.cat(total_input_parts, dim=1)\n",
        "\n",
        "            gates = torch.sigmoid(self.gate(total_input))\n",
        "            g2l_gate, l2g_gate = gates.chunk(2, dim=1)\n",
        "        else:\n",
        "            g2l_gate, l2g_gate = 1, 1\n",
        "\n",
        "        if self.ratio_gout != 1:\n",
        "            out_xl = self.convl2l(x_l) + self.convg2l(x_g) * g2l_gate\n",
        "        if self.ratio_gout != 0:\n",
        "            out_xg = self.convl2g(x_l) * l2g_gate + self.convg2g(x_g)\n",
        "\n",
        "        return out_xl, out_xg\n",
        "\n",
        "\n",
        "class FFC_BN_ACT(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels,\n",
        "                 kernel_size, ratio_gin, ratio_gout,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, bias=False,\n",
        "                 norm_layer=nn.BatchNorm2d, activation_layer=nn.Identity,\n",
        "                 padding_type='reflect',\n",
        "                 enable_lfu=True, **kwargs):\n",
        "        super(FFC_BN_ACT, self).__init__()\n",
        "        self.ffc = FFC(in_channels, out_channels, kernel_size,\n",
        "                       ratio_gin, ratio_gout, stride, padding, dilation,\n",
        "                       groups, bias, enable_lfu, padding_type=padding_type, **kwargs)\n",
        "\n",
        "        lnorm = nn.Identity if ratio_gout == 1 else norm_layer\n",
        "        gnorm = nn.Identity if ratio_gout == 0 else norm_layer\n",
        "        global_channels = int(out_channels * ratio_gout)\n",
        "        self.bn_l = lnorm(out_channels - global_channels)\n",
        "        self.bn_g = gnorm(global_channels)\n",
        "\n",
        "        lact = nn.Identity if ratio_gout == 1 else activation_layer\n",
        "        gact = nn.Identity if ratio_gout == 0 else activation_layer\n",
        "        self.act_l = lact(inplace=True)\n",
        "        self.act_g = gact(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_l, x_g = self.ffc(x)\n",
        "        x_l = self.act_l(self.bn_l(x_l))\n",
        "        x_g = self.act_g(self.bn_g(x_g))\n",
        "        return x_l, x_g"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "di = {'a':1}\n",
        "di['a'] += 1\n",
        "print(di)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-97DHNWjI5_",
        "outputId": "49b68a88-77f9-4ada-b74f-f9d9eca17b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rfiMnsQ-X52"
      },
      "source": [
        "## FFC RESNET BLOCKS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3xw7-B1-c15"
      },
      "outputs": [],
      "source": [
        "class FFCResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type, norm_layer, activation_layer=nn.ReLU, dilation=1,\n",
        "                 spatial_transform_kwargs=None, **conv_kwargs):\n",
        "        super().__init__()\n",
        "        self.conv1 = FFC_BN_ACT(dim, dim, kernel_size=3, padding=dilation, dilation=dilation,\n",
        "                                norm_layer=norm_layer,\n",
        "                                activation_layer=activation_layer,\n",
        "                                padding_type=padding_type,\n",
        "                                **conv_kwargs)\n",
        "        self.conv2 = FFC_BN_ACT(dim, dim, kernel_size=3, padding=dilation, dilation=dilation,\n",
        "                                norm_layer=norm_layer,\n",
        "                                activation_layer=activation_layer,\n",
        "                                padding_type=padding_type,\n",
        "                                **conv_kwargs)\n",
        "        if spatial_transform_kwargs is not None:\n",
        "            self.conv1 = LearnableSpatialTransformWrapper(self.conv1, **spatial_transform_kwargs)\n",
        "            self.conv2 = LearnableSpatialTransformWrapper(self.conv2, **spatial_transform_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_l, x_g = x if type(x) is tuple else (x, 0)\n",
        "\n",
        "        id_l, id_g = x_l, x_g\n",
        "\n",
        "        x_l, x_g = self.conv1((x_l, x_g))\n",
        "        x_l, x_g = self.conv2((x_l, x_g))\n",
        "\n",
        "        x_l, x_g = id_l + x_l, id_g + x_g\n",
        "        out = x_l, x_g\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConcatTupleLayer(nn.Module):\n",
        "    def forward(self, x):\n",
        "        assert isinstance(x, tuple)\n",
        "        x_l, x_g = x\n",
        "        assert torch.is_tensor(x_l) or torch.is_tensor(x_g)\n",
        "        if not torch.is_tensor(x_g):\n",
        "            return x_l\n",
        "        return torch.cat(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMObyBSw-iAJ"
      },
      "source": [
        "## FFC GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKTb0vr7-mol"
      },
      "outputs": [],
      "source": [
        "class FFCResNetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9,\n",
        "                 padding_type='reflect',spatial_transform_layers=None, max_features=1024):\n",
        "        assert (n_blocks >= 0)\n",
        "        super().__init__()\n",
        "\n",
        "        activation_layer=nn.ReLU\n",
        "        norm_layer=nn.BatchNorm2d\n",
        "        up_norm_layer=nn.BatchNorm2d\n",
        "        up_activation=nn.ReLU(True)\n",
        "\n",
        "        model = [nn.ReflectionPad2d(3),\n",
        "                 FFC_BN_ACT(input_nc, ngf, kernel_size=7, padding=0, norm_layer=norm_layer,\n",
        "                            activation_layer=activation_layer, **init_conv_kwargs)]\n",
        "\n",
        "        ### downsample\n",
        "\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** i\n",
        "            if i == n_downsampling - 1:\n",
        "                cur_conv_kwargs = dict(downsample_conv_kwargs)\n",
        "                cur_conv_kwargs['ratio_gout'] = resnet_conv_kwargs.get('ratio_gin', 0)\n",
        "            else:\n",
        "                cur_conv_kwargs = downsample_conv_kwargs\n",
        "            model += [FFC_BN_ACT(min(max_features, ngf * mult),\n",
        "                                 min(max_features, ngf * mult * 2),\n",
        "                                 kernel_size=3, stride=2, padding=1,\n",
        "                                 norm_layer=norm_layer,\n",
        "                                 activation_layer=activation_layer,\n",
        "                                 **cur_conv_kwargs)]\n",
        "\n",
        "        mult = 2 ** n_downsampling\n",
        "        feats_num_bottleneck = min(max_features, ngf * mult)\n",
        "\n",
        "        ### resnet blocks\n",
        "        for i in range(n_blocks):\n",
        "            cur_resblock = FFCResnetBlock(feats_num_bottleneck, padding_type=padding_type, activation_layer=activation_layer,\n",
        "                                          norm_layer=norm_layer, **resnet_conv_kwargs)\n",
        "            if spatial_transform_layers is not None and i in spatial_transform_layers:\n",
        "                cur_resblock = LearnableSpatialTransformWrapper(cur_resblock)\n",
        "            model += [cur_resblock]\n",
        "\n",
        "        model += [ConcatTupleLayer()]\n",
        "\n",
        "        ### upsample\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** (n_downsampling - i)\n",
        "            model += [nn.ConvTranspose2d(min(max_features, ngf * mult),\n",
        "                                         min(max_features, int(ngf * mult / 2)),\n",
        "                                         kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                      up_norm_layer(min(max_features, int(ngf * mult / 2))),\n",
        "                      up_activation]\n",
        "\n",
        "        model += [nn.ReflectionPad2d(3),nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
        "        model.append(nn.Tanh())\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvrxXq47AY54",
        "outputId": "43a0c272-874d-4baf-8bfe-6c018db47fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 104, 104])\n"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "    x = torch.randn((2,4,100, 100))\n",
        "    \n",
        "    model = FFCResNetGenerator(input_nc=4,output_nc=3)\n",
        "    pred = model(x)\n",
        "    print(pred.shape)\n",
        "test()      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrhmiovm-rNK"
      },
      "source": [
        "## FFC DISCRIMINAROR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaY9hGez-oRw"
      },
      "outputs": [],
      "source": [
        "class FFCNLayerDiscriminator(nn.Module):\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, max_features=512):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        def _act_ctor(inplace=True):\n",
        "            return nn.LeakyReLU(negative_slope=0.2, inplace=inplace)\n",
        "\n",
        "        kw = 3\n",
        "        padw = int(np.ceil((kw-1.0)/2))\n",
        "        sequence = [[FFC_BN_ACT(input_nc, ndf, kernel_size=kw, padding=padw, norm_layer=norm_layer,\n",
        "                                activation_layer=_act_ctor, **init_conv_kwargs)]]\n",
        "\n",
        "        nf = ndf\n",
        "        for n in range(1, n_layers):\n",
        "            nf_prev = nf\n",
        "            nf = min(nf * 2, max_features)\n",
        "\n",
        "            cur_model = [\n",
        "                FFC_BN_ACT(nf_prev, nf,\n",
        "                           kernel_size=kw, stride=2, padding=padw,\n",
        "                           norm_layer=norm_layer,\n",
        "                           activation_layer=_act_ctor,\n",
        "                           **conv_kwargs)\n",
        "            ]\n",
        "            sequence.append(cur_model)\n",
        "\n",
        "        nf_prev = nf\n",
        "        nf = min(nf * 2, 512)\n",
        "\n",
        "        cur_model = [\n",
        "            FFC_BN_ACT(nf_prev, nf,\n",
        "                       kernel_size=kw, stride=1, padding=padw,\n",
        "                       norm_layer=norm_layer,\n",
        "                       activation_layer=lambda *args, **kwargs: nn.LeakyReLU(*args, negative_slope=0.2, **kwargs),\n",
        "                       **conv_kwargs),\n",
        "            ConcatTupleLayer()\n",
        "        ]\n",
        "        sequence.append(cur_model)\n",
        "\n",
        "        sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\n",
        "\n",
        "        for n in range(len(sequence)):\n",
        "            setattr(self, 'model'+str(n), nn.Sequential(*sequence[n]))\n",
        "\n",
        "    def get_all_activations(self, x):\n",
        "        res = [x]\n",
        "        for n in range(self.n_layers + 2):\n",
        "            model = getattr(self, 'model' + str(n))\n",
        "            res.append(model(res[-1]))\n",
        "        return res[1:]\n",
        "\n",
        "    def forward(self, x):\n",
        "        act = self.get_all_activations(x)\n",
        "        feats = []\n",
        "        for out in act[:-1]:\n",
        "            if isinstance(out, tuple):\n",
        "                if torch.is_tensor(out[1]):\n",
        "                    out = torch.cat(out, dim=1)\n",
        "                else:\n",
        "                    out = out[0]\n",
        "            feats.append(out)\n",
        "        return act[-1], feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W75TS96-bv3d"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    x = torch.randn((2,3,100, 100))\n",
        "    \n",
        "    model = FFCNLayerDiscriminator(input_nc=3)\n",
        "    pred = model(x)\n",
        "test()   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0aWZnjhvfp"
      },
      "source": [
        "## LOSSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX2U4t_Jn7Kh"
      },
      "source": [
        "### RESNET BLOCKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzfXpWbyoAQ9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XDGjSc1k4r9"
      },
      "source": [
        "### RESNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZIkLIDik8Hk"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 128\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = conv3x3(3, 64, stride=2)\n",
        "        self.bn1 = BatchNorm2d(64)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(64, 64)\n",
        "        self.bn2 = BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv3 = conv3x3(64, 128)\n",
        "        self.bn3 = BatchNorm2d(128)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.bn1(self.conv1(x)))\n",
        "        x = self.relu2(self.bn2(self.conv2(x)))\n",
        "        x = self.relu3(self.bn3(self.conv3(x)))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def load_url(url, model_dir='./pretrained'):\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    filename = url.split('/')[-1]\n",
        "    cached_file = os.path.join(model_dir, filename)\n",
        "    if not os.path.exists(cached_file):\n",
        "        sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n",
        "        urlretrieve(url, cached_file)\n",
        "    return torch.load(cached_file,map_location=torch.device(DEVICE))\n",
        "\n",
        "def resnet50(pretrained=False):\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "    if pretrained:\n",
        "        model.load_state_dict(load_url('http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet50-imagenet.pth'), strict=False)\n",
        "    return model\n",
        "\n",
        "def resnet18(pretrained=False):\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "    if pretrained:\n",
        "        model.load_state_dict(load_url('http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet18-imagenet.pth'), strict=False)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpifU30xnvnE"
      },
      "source": [
        "### DILATEDRESNET (HRFPL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usbi7eEenzRs"
      },
      "outputs": [],
      "source": [
        "class ResnetDilated(nn.Module):\n",
        "    def __init__(self, orig_resnet):\n",
        "        super().__init__()\n",
        "\n",
        "        orig_resnet.layer3.apply(\n",
        "            partial(self._nostride_dilate, dilate=2))\n",
        "        orig_resnet.layer4.apply(\n",
        "            partial(self._nostride_dilate, dilate=4))\n",
        "\n",
        "        # take pretrained resnet, except AvgPool and FC\n",
        "        self.conv1 = orig_resnet.conv1\n",
        "        self.bn1 = orig_resnet.bn1\n",
        "        self.relu1 = orig_resnet.relu1\n",
        "        self.conv2 = orig_resnet.conv2\n",
        "        self.bn2 = orig_resnet.bn2\n",
        "        self.relu2 = orig_resnet.relu2\n",
        "        self.conv3 = orig_resnet.conv3\n",
        "        self.bn3 = orig_resnet.bn3\n",
        "        self.relu3 = orig_resnet.relu3\n",
        "        self.maxpool = orig_resnet.maxpool\n",
        "        self.layer1 = orig_resnet.layer1\n",
        "        self.layer2 = orig_resnet.layer2\n",
        "        self.layer3 = orig_resnet.layer3\n",
        "        self.layer4 = orig_resnet.layer4\n",
        "\n",
        "    def _nostride_dilate(self, m, dilate):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            # the convolution with stride\n",
        "            if m.stride == (2, 2):\n",
        "                m.stride = (1, 1)\n",
        "                if m.kernel_size == (3, 3):\n",
        "                    m.dilation = (dilate // 2, dilate // 2)\n",
        "                    m.padding = (dilate // 2, dilate // 2)\n",
        "            # other convoluions\n",
        "            else:\n",
        "                \n",
        "                if m.kernel_size == (3, 3):\n",
        "                    m.dilation = (dilate, dilate)\n",
        "                    m.padding = (dilate, dilate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = []\n",
        "\n",
        "        x = self.relu1(self.bn1(self.conv1(x)))\n",
        "        x = self.relu2(self.bn2(self.conv2(x)))\n",
        "        x = self.relu3(self.bn3(self.conv3(x)))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        conv_out.append(x)\n",
        "        x = self.layer2(x)\n",
        "        conv_out.append(x)\n",
        "        x = self.layer3(x)\n",
        "        conv_out.append(x)\n",
        "        x = self.layer4(x)\n",
        "        conv_out.append(x)\n",
        "\n",
        "        return conv_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4E0nb1SpPGb"
      },
      "outputs": [],
      "source": [
        "IMAGENET_MEAN = torch.FloatTensor([0.485, 0.456, 0.406])[None, :, None, None]\n",
        "IMAGENET_STD = torch.FloatTensor([0.229, 0.224, 0.225])[None, :, None, None]\n",
        "\n",
        "class ResNetPL(nn.Module):\n",
        "    def __init__(self, weight=30):\n",
        "        super().__init__()\n",
        "        origResnet = resnet50(pretrained=True).to(DEVICE)\n",
        "        self.impl = ResnetDilated(origResnet).to(DEVICE)\n",
        "        self.impl.eval()\n",
        "        for w in self.impl.parameters():\n",
        "            w.requires_grad_(False)\n",
        "\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = (pred - IMAGENET_MEAN.to(DEVICE)) / IMAGENET_STD.to(DEVICE)\n",
        "        target = (target - IMAGENET_MEAN.to(DEVICE)) / IMAGENET_STD.to(DEVICE)\n",
        "\n",
        "        pred_feats = self.impl(pred)\n",
        "        target_feats = self.impl(target)\n",
        "\n",
        "        result = torch.stack([F.mse_loss(cur_pred, cur_target)\n",
        "                              for cur_pred, cur_target\n",
        "                              in zip(pred_feats, target_feats)]).sum() * self.weight\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5f8E_NIn4cT",
        "outputId": "e1f79c1d-c356-4c7a-9e12-dadfc03920a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet50-imagenet.pth\" to ./pretrained/resnet50-imagenet.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(370.5209)\n"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "    x = torch.randn((2,3,256, 256)).to(DEVICE)\n",
        "    y = torch.randn((2,3,256, 256)).to(DEVICE)\n",
        "    model = ResNetPL().to(DEVICE)\n",
        "    pred = model(x,y)\n",
        "    print(pred)\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9W26j0jt3N5"
      },
      "source": [
        "### R1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts-YPJK2t2he"
      },
      "outputs": [],
      "source": [
        "def make_r1_gp(discr_real_pred, real_batch):\n",
        "    if torch.is_grad_enabled():\n",
        "        grad_real = torch.autograd.grad(outputs=discr_real_pred.sum(), inputs=real_batch, create_graph=True)[0]\n",
        "        grad_penalty = (grad_real.view(grad_real.shape[0], -1).norm(2, dim=1) ** 2).mean()\n",
        "    else:\n",
        "        grad_penalty = 0\n",
        "    real_batch.requires_grad = False\n",
        "\n",
        "    return grad_penalty\n",
        "\n",
        "class NonSaturatingWithR1():\n",
        "    def __init__(self, gp_coef=0.001, weight=10):\n",
        "      \n",
        "        self.gp_coef = gp_coef\n",
        "        self.weight = weight\n",
        "\n",
        "    def interpolate_mask(self, mask, shape):\n",
        "        if shape != mask.shape[-2:]:\n",
        "            mask = F.interpolate(mask, size=shape, mode='nearest')\n",
        "        return mask\n",
        "\n",
        "    def generator_loss(self, discr_fake_pred):\n",
        "\n",
        "        fake_loss = F.softplus(-discr_fake_pred)\n",
        "        return fake_loss.mean() * self.weight\n",
        "\n",
        "    def discriminator_loss(self, real_batch: torch.Tensor, fake_batch: torch.Tensor,\n",
        "                           discr_real_pred: torch.Tensor, discr_fake_pred: torch.Tensor,\n",
        "                           mask=None) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "\n",
        "        real_loss = F.softplus(-discr_real_pred)\n",
        "        grad_penalty = make_r1_gp(discr_real_pred, real_batch) * self.gp_coef\n",
        "        fake_loss = F.softplus(discr_fake_pred)\n",
        "\n",
        "        mask = self.interpolate_mask(mask, discr_fake_pred.shape[-2:])\n",
        "        fake_loss = fake_loss * mask\n",
        "        fake_loss = fake_loss + (1 - mask) * F.softplus(-discr_fake_pred)\n",
        "\n",
        "        sum_discr_loss = real_loss + grad_penalty + fake_loss\n",
        "        return sum_discr_loss.mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsknvOLpTnXh"
      },
      "source": [
        "### L1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi3DawBHTqgM"
      },
      "outputs": [],
      "source": [
        "def masked_l1_loss(pred, target, mask, weight_known, weight_missing):\n",
        "    per_pixel_l1 = F.l1_loss(pred, target, reduction='none')\n",
        "    pixel_weights = mask * weight_missing + (1 - mask) * weight_known\n",
        "    return (pixel_weights * per_pixel_l1).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv2N1s0ljjdP"
      },
      "source": [
        "### FEATURE MATCHIN (DiscPL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KyWNq8CjgQW"
      },
      "outputs": [],
      "source": [
        "def feature_matching_loss(fake_features: List[torch.Tensor], target_features: List[torch.Tensor], mask=None):\n",
        "    if mask is None:\n",
        "        res = torch.stack([F.mse_loss(fake_feat, target_feat)\n",
        "                           for fake_feat, target_feat in zip(fake_features, target_features)]).mean()\n",
        "    else:\n",
        "        res = 0\n",
        "        norm = 0\n",
        "        for fake_feat, target_feat in zip(fake_features, target_features):\n",
        "            cur_mask = F.interpolate(mask, size=fake_feat.shape[-2:], mode='bilinear', align_corners=False)\n",
        "            error_weights = 1 - cur_mask\n",
        "            cur_val = ((fake_feat - target_feat).pow(2) * error_weights).mean()\n",
        "            res = res + cur_val\n",
        "            norm += 1\n",
        "        res = res / norm\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IrYaeC2sFpW"
      },
      "source": [
        "### FULL LOSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krmTF7OgXlWe"
      },
      "outputs": [],
      "source": [
        "adversarial_loss = NonSaturatingWithR1()\n",
        "def generator_loss(img, predicted_img, original_mask, discriminator):\n",
        "  # L1\n",
        "  l1_value = masked_l1_loss(predicted_img, img, original_mask,10,0) \n",
        "  total_loss = l1_value \n",
        "\n",
        "  # discriminator\n",
        "  img.requires_grad = True\n",
        "  discr_real_pred, discr_real_features = discriminator(img)\n",
        "  discr_fake_pred, discr_fake_features = discriminator(predicted_img)\n",
        "  adv_gen_loss = adversarial_loss.generator_loss(discr_fake_pred)\n",
        "  \n",
        "  total_loss = total_loss + adv_gen_loss  \n",
        "  # feature matching\n",
        "  feature_matching_weight = 100\n",
        "  fm_value = feature_matching_loss(discr_fake_features, discr_real_features,\n",
        "                              mask=original_mask) * feature_matching_weight\n",
        "  total_loss = total_loss + fm_value  \n",
        "  loss_resnet_pl = ResNetPL().to(DEVICE)\n",
        "  resnet_pl_value = loss_resnet_pl(predicted_img, img)\n",
        "  total_loss = total_loss + resnet_pl_value \n",
        "\n",
        "  return total_loss\n",
        "\n",
        "def discriminator_loss(img,predicted_img,mask,discriminator):\n",
        "  img.requires_grad = True\n",
        "  discr_real_pred, discr_real_features = discriminator(img)\n",
        "  discr_fake_pred, discr_fake_features = discriminator(predicted_img.detach())\n",
        "  adv_discr_loss = adversarial_loss.discriminator_loss(img,\n",
        "                                                       predicted_img,\n",
        "                                                       discr_real_pred,\n",
        "                                                       discr_fake_pred,\n",
        "                                                       mask)\n",
        "  return adv_discr_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_inventory(dct):\n",
        "    print(\"Items held:\")\n",
        "    for item, amount in dct.items():  # dct.iteritems() in Python 2\n",
        "        print(\"{} ({})\".format(item, amount))"
      ],
      "metadata": {
        "id": "dJQ86_n8pZC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh7RRgZJj48N"
      },
      "source": [
        "# LAMA TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZycXyfjIpz5"
      },
      "outputs": [],
      "source": [
        "psnr = PeakSignalNoiseRatio().to(device=DEVICE)\n",
        "ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device=DEVICE)\n",
        "\n",
        "def save_LAMA(model, gen_checkpoint,disc_checkpoint):\n",
        "    save_checkpoint(model,gen_checkpoint,'gen_checkpoint.pth.tar')\n",
        "    save_checkpoint(model,disc_checkpoint,'disc_checkpoint.pth.tar')\n",
        "    wandb.log_artifact(model)\n",
        "\n",
        "def gan_train_fn(loader, gen, disc, gen_optimizer, disc_optimizer, generator_loss,\n",
        "                 discriminator_loss,train_generator):\n",
        "    loop = tqdm(loader)\n",
        "    sum_gen_loss = 0\n",
        "    sum_disc_loss = 0\n",
        "    for batch_idx, (image, mask, masked_image) in enumerate(loop):\n",
        "\n",
        "        masked_image = masked_image.to(device=DEVICE)\n",
        "        image = image.to(device=DEVICE)\n",
        "        mask = mask.to(device=DEVICE)\n",
        "        masked_img = torch.cat([masked_image, mask], dim=1)\n",
        "        masked_img = masked_img.to(device=DEVICE)\n",
        "\n",
        "        pred = gen(masked_img)\n",
        "        disc_loss = discriminator_loss(image, pred, mask, disc)\n",
        "        gen_loss = generator_loss(image, pred, mask, disc)\n",
        "\n",
        "        PSNR = psnr(pred,image).item()\n",
        "        SSIM = ssim(pred,image).item()\n",
        "        \n",
        "        if train_generator:\n",
        "          sum_gen_loss += gen_loss.item()\n",
        "          gen_optimizer.zero_grad()\n",
        "          gen_loss.backward()\n",
        "          gen_optimizer.step()\n",
        "        else:\n",
        "          sum_disc_loss += disc_loss.item()\n",
        "          disc_optimizer.zero_grad()\n",
        "          disc_loss.backward()\n",
        "          disc_optimizer.step()\n",
        "        loop.set_postfix(gen_loss=gen_loss.item(),\n",
        "                         disc_loss=disc_loss.item(),\n",
        "                         PSNR=PSNR,\n",
        "                         SSIM=SSIM)\n",
        "\n",
        "    wandb.log({\"train_loss_gen_mean\": sum_gen_loss/len(loop)})\n",
        "    wandb.log({\"train_loss_disc_mean\": sum_disc_loss/len(loop)})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi\n",
        "# !pip install numba\n",
        "# from numba import cuda\n",
        "# cuda.select_device(0)\n",
        "# cuda.close()\n",
        "# !nvidia-smi  "
      ],
      "metadata": {
        "id": "3cyHKliepdBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZwx4Br-I5jS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690,
          "referenced_widgets": [
            "37fd76887887412783be78d0354ef0a7",
            "959cceb58414455e81553e4b2b816e9f",
            "3a164ead9f2f4118967d30551ff9e1c8",
            "ca9d79ee83214ff492eb4d0b798f67b0",
            "35b51985e7414503ab195b1f6ead3728",
            "52b4ec395bf94ac4a0a45f360b896d55",
            "9a45fcb46ca048cf8636b8795b6cc7b3",
            "6474340aa15940b1b11d5efc67d2a914"
          ]
        },
        "outputId": "64c792c9-cf6b-4cf9-950d-570629e49daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrkozliak\u001b[0m (\u001b[33mrkozlyak\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230221_030849-beutla9y</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rkozlyak/WatermarkRemoval/runs/beutla9y' target=\"_blank\">experiment_0</a></strong> to <a href='https://wandb.ai/rkozlyak/WatermarkRemoval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rkozlyak/WatermarkRemoval' target=\"_blank\">https://wandb.ai/rkozlyak/WatermarkRemoval</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rkozlyak/WatermarkRemoval/runs/beutla9y' target=\"_blank\">https://wandb.ai/rkozlyak/WatermarkRemoval/runs/beutla9y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact lama_model_50_2:latest, 316.09MB. 2 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
            "Done. 0:0:4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37fd76887887412783be78d0354ef0a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_0</strong> at: <a href='https://wandb.ai/rkozlyak/WatermarkRemoval/runs/beutla9y' target=\"_blank\">https://wandb.ai/rkozlyak/WatermarkRemoval/runs/beutla9y</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230221_030849-beutla9y/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f8a19e9513ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-f8a19e9513ac>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0martifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rkozlyak/WatermarkRemoval/lama_model_50_2:latest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0martifact_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/gen_checkpoint.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/disc_checkpoint.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         loaded_storages[key] = torch.storage.TypedStorage(\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m             dtype=dtype)\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    167\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ],
      "source": [
        "discriminator = FFCNLayerDiscriminator(input_nc=3).to(DEVICE)\n",
        "generator = FFCResNetGenerator(4,3).to(DEVICE)\n",
        "\n",
        "generator_loss = generator_loss\n",
        "discriminator_loss = discriminator_loss\n",
        "gen_optimizer = optim.Adam(generator.parameters(), lr=GEN_LEARNING_RATE)\n",
        "disc_optimizer = optim.Adam(discriminator.parameters(), lr=DISC_LEARNING_RATE)\n",
        "\n",
        "def wandb_init(epoch):\n",
        "    return wandb.init(\n",
        "    reinit=True,\n",
        "    project=\"WatermarkRemoval\", \n",
        "    name=f\"experiment_{epoch}\",\n",
        "    config = config,\n",
        "    entity=\"rkozlyak\")\n",
        "\n",
        "def main():\n",
        "    \n",
        "    for epoch in range(0,NUM_EPOCHS):\n",
        "      with wandb_init(epoch) as run:\n",
        "        model_artifact_name = \"lama_model_50_2\"\n",
        "        model_artifact = wandb.Artifact(model_artifact_name, type='model')\n",
        "\n",
        "        if True:\n",
        "            artifact = wandb.use_artifact('rkozlyak/WatermarkRemoval/lama_model_50_2:latest', type='model')\n",
        "            artifact_dir = artifact.download()\n",
        "            load_checkpoint(torch.load(artifact_dir + '/gen_checkpoint.pth.tar'), generator)\n",
        "            load_checkpoint(torch.load(artifact_dir + '/disc_checkpoint.pth.tar'), discriminator)\n",
        "\n",
        "        train_generator = epoch%2==0\n",
        "        if train_generator:\n",
        "          print('train_generator')\n",
        "        else:\n",
        "          print('train_discriminator')\n",
        "\n",
        "        gan_train_fn(train_loader, generator,discriminator, gen_optimizer,disc_optimizer, generator_loss,discriminator_loss,train_generator)\n",
        "        check_accuracy(val_loader, generator,discriminator, gen_optimizer,disc_optimizer, generator_loss)\n",
        "\n",
        "        gen_checkpoint =  make_checkpoint(generator,gen_optimizer)\n",
        "        disc_checkpoint =  make_checkpoint(discriminator,disc_optimizer)\n",
        "\n",
        "        save_LAMA(model_artifact,gen_checkpoint,disc_checkpoint)\n",
        "        run.finish()\n",
        "    wandb.finish()\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disc = FFCNLayerDiscriminator(input_nc=3).to(DEVICE)\n",
        "gen = FFCResNetGenerator(4,3).to(DEVICE)\n",
        "gen_opt = optim.Adam(disc.parameters(), lr=GEN_LEARNING_RATE)\n",
        "disc_opt = optim.Adam(gen.parameters(), lr=DISC_LEARNING_RATE)\n",
        "wandb.finish()\n",
        "with wandb_init(4) as run:\n",
        "  gen_checkpoint =  make_checkpoint(gen,gen_opt)\n",
        "  print(gen_checkpoint)\n",
        "  disc_checkpoint =  make_checkpoint(disc,disc_opt)\n",
        "  save_LAMA(gen_checkpoint,disc_checkpoint)\n",
        "with wandb_init(4) as run:\n",
        "  artifact = wandb.use_artifact('rkozlyak/WatermarkRemoval/lama_model:latest', type='model')\n",
        "  path = artifact.download()\n",
        "  print(path)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "YXOjLLAvyfnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lve1WAZu7qnv"
      },
      "source": [
        "# CM-GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QieVKs-v67H"
      },
      "source": [
        "## FFC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wgiy_jHtrt5"
      },
      "outputs": [],
      "source": [
        "class FFCSE_block(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, ratio_g):\n",
        "        super(FFCSE_block, self).__init__()\n",
        "        in_cg = int(channels * ratio_g)\n",
        "        in_cl = channels - in_cg\n",
        "        r = 16\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.conv1 = nn.Conv2d(channels, channels // r,\n",
        "                               kernel_size=1, bias=True)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv_a2l = None if in_cl == 0 else nn.Conv2d(\n",
        "            channels // r, in_cl, kernel_size=1, bias=True)\n",
        "        self.conv_a2g = None if in_cg == 0 else nn.Conv2d(\n",
        "            channels // r, in_cg, kernel_size=1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x if type(x) is tuple else (x, 0)\n",
        "        id_l, id_g = x\n",
        "        print(id_l.shape)\n",
        "        x = id_l if type(id_g) is int else torch.cat([id_l, id_g], dim=1)\n",
        "        print(x.shape)\n",
        "        x = self.avgpool(x)\n",
        "        print(x.shape)\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        print(x.shape)\n",
        "\n",
        "        x_l = 0 if self.conv_a2l is None else id_l * \\\n",
        "            self.sigmoid(self.conv_a2l(x))\n",
        "        x_g = 0 if self.conv_a2g is None else id_g * \\\n",
        "            self.sigmoid(self.conv_a2g(x))\n",
        "        return x_l, x_g\n",
        "\n",
        "class FourierUnit(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, groups=1):\n",
        "        # bn_layer not used\n",
        "        super(FourierUnit, self).__init__()\n",
        "        self.groups = groups\n",
        "        self.conv_layer = torch.nn.Conv2d(in_channels=in_channels * 2, out_channels=out_channels * 2,\n",
        "                                          kernel_size=1, stride=1, padding=0, groups=self.groups, bias=False)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels * 2)\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, c, h, w = x.size()\n",
        "        r_size = x.size()\n",
        "\n",
        "        # (batch, c, h, w/2+1, 2)\n",
        "        ffted = torch.rfft(x, signal_ndim=2, normalized=True)\n",
        "        # (batch, c, 2, h, w/2+1)\n",
        "        ffted = ffted.permute(0, 1, 4, 2, 3).contiguous()\n",
        "        ffted = ffted.view((batch, -1,) + ffted.size()[3:])\n",
        "\n",
        "        ffted = self.conv_layer(ffted)  # (batch, c*2, h, w/2+1)\n",
        "        ffted = self.relu(self.bn(ffted))\n",
        "\n",
        "        ffted = ffted.view((batch, -1, 2,) + ffted.size()[2:]).permute(\n",
        "            0, 1, 3, 4, 2).contiguous()  # (batch,c, t, h, w/2+1, 2)\n",
        "\n",
        "        output = torch.irfft(ffted, signal_ndim=2,\n",
        "                             signal_sizes=r_size[2:], normalized=True)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class SpectralTransform(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, groups=1, enable_lfu=True):\n",
        "        # bn_layer not used\n",
        "        super(SpectralTransform, self).__init__()\n",
        "        self.enable_lfu = enable_lfu\n",
        "        if stride == 2:\n",
        "            self.downsample = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n",
        "        else:\n",
        "            self.downsample = nn.Identity()\n",
        "\n",
        "        self.stride = stride\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels //\n",
        "                      2, kernel_size=1, groups=groups, bias=False),\n",
        "            nn.BatchNorm2d(out_channels // 2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.fu = FourierUnit(\n",
        "            out_channels // 2, out_channels // 2, groups)\n",
        "        if self.enable_lfu:\n",
        "            self.lfu = FourierUnit(\n",
        "                out_channels // 2, out_channels // 2, groups)\n",
        "        self.conv2 = torch.nn.Conv2d(\n",
        "            out_channels // 2, out_channels, kernel_size=1, groups=groups, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.downsample(x)\n",
        "        x = self.conv1(x)\n",
        "        output = self.fu(x)\n",
        "\n",
        "        if self.enable_lfu:\n",
        "            n, c, h, w = x.shape\n",
        "            split_no = 2\n",
        "            split_s_h = h // split_no\n",
        "            split_s_w = w // split_no\n",
        "            xs = torch.cat(torch.split(\n",
        "                x[:, :c // 4], split_s_h, dim=-2), dim=1).contiguous()\n",
        "            xs = torch.cat(torch.split(xs, split_s_w, dim=-1),\n",
        "                           dim=1).contiguous()\n",
        "            xs = self.lfu(xs)\n",
        "            xs = xs.repeat(1, 1, split_no, split_no).contiguous()\n",
        "        else:\n",
        "            xs = 0\n",
        "\n",
        "        output = self.conv2(x + output + xs)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class FFC(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 ratio_gin, ratio_gout, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=False, enable_lfu=True):\n",
        "        super(FFC, self).__init__()\n",
        "\n",
        "        assert stride == 1 or stride == 2, \"Stride should be 1 or 2.\"\n",
        "        self.stride = stride\n",
        "\n",
        "        in_cg = int(in_channels * ratio_gin)\n",
        "        print(in_cg)\n",
        "        in_cl = in_channels - in_cg\n",
        "        print(in_cl)\n",
        "        out_cg = int(out_channels * ratio_gout)\n",
        "        print(out_cg)\n",
        "        out_cl = out_channels - out_cg\n",
        "        print(out_cl)\n",
        "        #groups_g = 1 if groups == 1 else int(groups * ratio_gout)\n",
        "        #groups_l = 1 if groups == 1 else groups - groups_g\n",
        "\n",
        "        self.ratio_gin = ratio_gin\n",
        "        self.ratio_gout = ratio_gout\n",
        "\n",
        "        module = nn.Identity if in_cl == 0 or out_cl == 0 else nn.Conv2d\n",
        "        self.convl2l = module(in_cl, out_cl, kernel_size,\n",
        "                              stride, padding, dilation, groups, bias)\n",
        "        module = nn.Identity if in_cl == 0 or out_cg == 0 else nn.Conv2d\n",
        "        self.convl2g = module(in_cl, out_cg, kernel_size,\n",
        "                              stride, padding, dilation, groups, bias)\n",
        "        module = nn.Identity if in_cg == 0 or out_cl == 0 else nn.Conv2d\n",
        "        self.convg2l = module(in_cg, out_cl, kernel_size,\n",
        "                              stride, padding, dilation, groups, bias)\n",
        "        module = nn.Identity if in_cg == 0 or out_cg == 0 else SpectralTransform\n",
        "        self.convg2g = module(\n",
        "            in_cg, out_cg, stride, 1 if groups == 1 else groups // 2, enable_lfu)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_l, x_g = x if type(x) is tuple else (x, 0)\n",
        "        out_xl, out_xg = 0, 0\n",
        "\n",
        "        if self.ratio_gout != 1:\n",
        "            out_xl = self.convl2l(x_l) + self.convg2l(x_g)\n",
        "        if self.ratio_gout != 0:\n",
        "            out_xg = self.convl2g(x_l) + self.convg2g(x_g)\n",
        "\n",
        "        return out_xl, out_xg\n",
        "\n",
        "\n",
        "class FFC_BN_ACT(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels,\n",
        "                 kernel_size, ratio_gin, ratio_gout,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, bias=False,\n",
        "                 norm_layer=nn.BatchNorm2d, activation_layer=nn.Identity,\n",
        "                 enable_lfu=True):\n",
        "        super(FFC_BN_ACT, self).__init__()\n",
        "        self.ffc = FFC(in_channels, out_channels, kernel_size,\n",
        "                       ratio_gin, ratio_gout, stride, padding, dilation,\n",
        "                       groups, bias, enable_lfu)\n",
        "        lnorm = nn.Identity if ratio_gout == 1 else norm_layer\n",
        "        gnorm = nn.Identity if ratio_gout == 0 else norm_layer\n",
        "        self.bn_l = lnorm(int(out_channels * (1 - ratio_gout)))\n",
        "        self.bn_g = gnorm(int(out_channels * ratio_gout))\n",
        "\n",
        "        lact = nn.Identity if ratio_gout == 1 else activation_layer\n",
        "        gact = nn.Identity if ratio_gout == 0 else activation_layer\n",
        "        self.act_l = lact(inplace=True)\n",
        "        self.act_g = gact(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_l, x_g = self.ffc(x)\n",
        "        x_l = self.act_l(self.bn_l(x_l))\n",
        "        x_g = self.act_g(self.bn_g(x_g))\n",
        "        return x_l, x_g\n",
        "\n",
        "def test():\n",
        "    x = torch.randn((2, 3, 512, 512))\n",
        "    # block = FFCSE_block(3,1)\n",
        "    # bl = block(x)\n",
        "    # print(bl)\n",
        "    model = FFC_BN_ACT(3, 3,3,0,1)\n",
        "    x_l,x_g = model(x)\n",
        "    print(x.shape)\n",
        "    print(x_g)\n",
        "    \n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqFnAW-57h_f"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd0udglQ7oJh",
        "outputId": "003807c7-c34f-4c0b-9b1b-deba2149985f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 512])\n"
          ]
        }
      ],
      "source": [
        "class EqualLinear(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, lr_mul = 1, bias = True):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_dim))\n",
        "\n",
        "        self.lr_mul = lr_mul\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self.weight * self.lr_mul, bias=self.bias * self.lr_mul)\n",
        "\n",
        "class StyleVectorizer(nn.Module):\n",
        "    def __init__(self, emb, depth, lr_mul = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(depth):\n",
        "            layers.extend([EqualLinear(emb, emb, lr_mul), nn.LeakyReLU(0.2, inplace=True)])\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.normalize(x, dim=1)\n",
        "        return self.net(x)\n",
        "\n",
        "def test():\n",
        "    x = torch.randn((2, 512))\n",
        "    model = StyleVectorizer(512,10)\n",
        "    y = model(x)\n",
        "    print(y.shape)\n",
        "    \n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHL42_BFaZtM"
      },
      "source": [
        "## MODULETION BLOCK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AszEe3oPahPd",
        "outputId": "0ddf0bb4-1b42-43e2-c69a-5e761c24a248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "torch.Size([1, 6, 512, 512])\n",
            "torch.Size([1, 6, 512, 512])\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "class Conv2DMod(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, kernel, demod=True, stride=1, dilation=1, eps = 1e-8, **kwargs):\n",
        "        super().__init__()\n",
        "        self.out_chan = out_chan\n",
        "        self.demod = demod\n",
        "        self.kernel = kernel\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "        self.weight = nn.Parameter(torch.randn((out_chan, in_chan, kernel, kernel)))\n",
        "        self.eps = eps\n",
        "        nn.init.kaiming_normal_(self.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "    def _get_same_padding(self, size, kernel, dilation, stride):\n",
        "        return ((size - 1) * (stride - 1) + dilation * (kernel - 1)) // 2\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        b, c, h, w = x.shape\n",
        "        w1 = y[:, None, :, None, None]\n",
        "        w2 = self.weight[None, :, :, :, :]\n",
        "        weights = w2 * (w1 + 1)\n",
        "\n",
        "        if self.demod:\n",
        "            d = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n",
        "            weights = weights * d\n",
        "\n",
        "        x = x.reshape(1, -1, h, w)\n",
        "        _, _, *ws = weights.shape\n",
        "        weights = weights.reshape(b * self.out_chan, *ws)\n",
        "\n",
        "        padding = self._get_same_padding(h, self.kernel, self.dilation, self.stride)\n",
        "        print(padding)\n",
        "        print(x.shape)\n",
        "        x = F.conv2d(x, weights, padding=padding, groups=b)\n",
        "        print(x.shape)\n",
        "        x = x.reshape(-1, self.out_chan, h, w)\n",
        "        print(x.shape)\n",
        "        return x\n",
        "\n",
        "def test():\n",
        "    x = torch.randn((2,3,512, 512))\n",
        "    y = torch.randn((2,3))\n",
        "    \n",
        "    model = Conv2DMod(in_chan=3,out_chan=3,kernel=3)\n",
        "    z = model(x,y)\n",
        "    print(y.shape)\n",
        "    \n",
        "test()        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-jnk1SZxBk7",
        "outputId": "77cd885e-25c1-43ab-b6fc-6dad1ab507a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2])\n",
            "torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "a = torch.arange(4.)\n",
        "torch.reshape(a, (2, 2))\n",
        "b = torch.tensor([[0, 1], [2, 3]])\n",
        "print(b.shape)\n",
        "b = torch.reshape(b, (-1,))\n",
        "print(b.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHf1YyrH9vsF"
      },
      "source": [
        "# CONTEXTUAL ATTENTION GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V28EHkvT5VKE"
      },
      "source": [
        "## CONTEXTUAL UTILS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T2Q4pI0os83"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "\n",
        "def default_loader(path):\n",
        "    return pil_loader(path)\n",
        "\n",
        "\n",
        "def tensor_img_to_npimg(tensor_img):\n",
        "    \"\"\"\n",
        "    Turn a tensor image with shape CxHxW to a numpy array image with shape HxWxC\n",
        "    :param tensor_img:\n",
        "    :return: a numpy array image with shape HxWxC\n",
        "    \"\"\"\n",
        "    if not (torch.is_tensor(tensor_img) and tensor_img.ndimension() == 3):\n",
        "        raise NotImplementedError(\"Not supported tensor image. Only tensors with dimension CxHxW are supported.\")\n",
        "    npimg = np.transpose(tensor_img.numpy(), (1, 2, 0))\n",
        "    npimg = npimg.squeeze()\n",
        "    assert isinstance(npimg, np.ndarray) and (npimg.ndim in {2, 3})\n",
        "    return npimg\n",
        "\n",
        "\n",
        "# Change the values of tensor x from range [0, 1] to [-1, 1]\n",
        "def normalize(x):\n",
        "    return x.mul_(2).add_(-1)\n",
        "\n",
        "def same_padding(images, ksizes, strides, rates):\n",
        "    assert len(images.size()) == 4\n",
        "    batch_size, channel, rows, cols = images.size()\n",
        "    out_rows = (rows + strides[0] - 1) // strides[0]\n",
        "    out_cols = (cols + strides[1] - 1) // strides[1]\n",
        "    effective_k_row = (ksizes[0] - 1) * rates[0] + 1\n",
        "    effective_k_col = (ksizes[1] - 1) * rates[1] + 1\n",
        "    padding_rows = max(0, (out_rows-1)*strides[0]+effective_k_row-rows)\n",
        "    padding_cols = max(0, (out_cols-1)*strides[1]+effective_k_col-cols)\n",
        "    # Pad the input\n",
        "    padding_top = int(padding_rows / 2.)\n",
        "    padding_left = int(padding_cols / 2.)\n",
        "    padding_bottom = padding_rows - padding_top\n",
        "    padding_right = padding_cols - padding_left\n",
        "    paddings = (padding_left, padding_right, padding_top, padding_bottom)\n",
        "    images = torch.nn.ZeroPad2d(paddings)(images)\n",
        "    return images\n",
        "\n",
        "\n",
        "def extract_image_patches(images, ksizes, strides, rates, padding='same'):\n",
        "    \"\"\"\n",
        "    Extract patches from images and put them in the C output dimension.\n",
        "    :param padding:\n",
        "    :param images: [batch, channels, in_rows, in_cols]. A 4-D Tensor with shape\n",
        "    :param ksizes: [ksize_rows, ksize_cols]. The size of the sliding window for\n",
        "     each dimension of images\n",
        "    :param strides: [stride_rows, stride_cols]\n",
        "    :param rates: [dilation_rows, dilation_cols]\n",
        "    :return: A Tensor\n",
        "    \"\"\"\n",
        "    assert len(images.size()) == 4\n",
        "    assert padding in ['same', 'valid']\n",
        "    batch_size, channel, height, width = images.size()\n",
        "\n",
        "    if padding == 'same':\n",
        "        images = same_padding(images, ksizes, strides, rates)\n",
        "    elif padding == 'valid':\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError('Unsupported padding type: {}.\\\n",
        "                Only \"same\" or \"valid\" are supported.'.format(padding))\n",
        "\n",
        "    unfold = torch.nn.Unfold(kernel_size=ksizes,\n",
        "                             dilation=rates,\n",
        "                             padding=0,\n",
        "                             stride=strides)\n",
        "    patches = unfold(images)\n",
        "    return patches  # [N, C*k*k, L], L is the total number of such blocks\n",
        "\n",
        "\n",
        "def random_bbox(config, batch_size):\n",
        "    \"\"\"Generate a random tlhw with configuration.\n",
        "\n",
        "    Args:\n",
        "        config: Config should have configuration including img\n",
        "\n",
        "    Returns:\n",
        "        tuple: (top, left, height, width)\n",
        "\n",
        "    \"\"\"\n",
        "    img_height, img_width, _ = config['image_shape']\n",
        "    h, w = config['mask_shape']\n",
        "    margin_height, margin_width = config['margin']\n",
        "    maxt = img_height - margin_height - h\n",
        "    maxl = img_width - margin_width - w\n",
        "    bbox_list = []\n",
        "    if config['mask_batch_same']:\n",
        "        t = np.random.randint(margin_height, maxt)\n",
        "        l = np.random.randint(margin_width, maxl)\n",
        "        bbox_list.append((t, l, h, w))\n",
        "        bbox_list = bbox_list * batch_size\n",
        "    else:\n",
        "        for i in range(batch_size):\n",
        "            t = np.random.randint(margin_height, maxt)\n",
        "            l = np.random.randint(margin_width, maxl)\n",
        "            bbox_list.append((t, l, h, w))\n",
        "\n",
        "    return torch.tensor(bbox_list, dtype=torch.int64)\n",
        "\n",
        "\n",
        "\n",
        "def bbox2mask(bboxes, height, width, max_delta_h, max_delta_w):\n",
        "    batch_size = bboxes.size(0)\n",
        "    mask = torch.zeros((batch_size, 1, height, width), dtype=torch.float32)\n",
        "    for i in range(batch_size):\n",
        "        bbox = bboxes[i]\n",
        "        delta_h = np.random.randint(max_delta_h // 2 + 1)\n",
        "        delta_w = np.random.randint(max_delta_w // 2 + 1)\n",
        "        mask[i, :, bbox[0] + delta_h:bbox[0] + bbox[2] - delta_h, bbox[1] + delta_w:bbox[1] + bbox[3] - delta_w] = 1.\n",
        "    return mask\n",
        "\n",
        "\n",
        "def test_bbox2mask():\n",
        "    image_shape = [256, 256, 3]\n",
        "    mask_shape = [128, 128]\n",
        "    margin = [0, 0]\n",
        "    max_delta_shape = [32, 32]\n",
        "    bbox = random_bbox(image_shape)\n",
        "    mask = bbox2mask(bbox, image_shape[0], image_shape[1], max_delta_shape[0], max_delta_shape[1])\n",
        "    return mask\n",
        "\n",
        "\n",
        "def local_patch(x, bbox_list):\n",
        "    assert len(x.size()) == 4\n",
        "    patches = []\n",
        "    for i, bbox in enumerate(bbox_list):\n",
        "        t, l, h, w = bbox\n",
        "        patches.append(x[i, :, t:t + h, l:l + w])\n",
        "    return torch.stack(patches, dim=0)\n",
        "\n",
        "\n",
        "def mask_image(x, bboxes, config):\n",
        "    height, width, _ = config['image_shape']\n",
        "    max_delta_h, max_delta_w = config['max_delta_shape']\n",
        "    mask = bbox2mask(bboxes, height, width, max_delta_h, max_delta_w)\n",
        "    if x.is_cuda:\n",
        "        mask = mask.cuda()\n",
        "\n",
        "    if config['mask_type'] == 'hole':\n",
        "        result = x * (1. - mask)\n",
        "    elif config['mask_type'] == 'mosaic':\n",
        "        # TODO: Matching the mosaic patch size and the mask size\n",
        "        mosaic_unit_size = config['mosaic_unit_size']\n",
        "        downsampled_image = F.interpolate(x, scale_factor=1. / mosaic_unit_size, mode='nearest')\n",
        "        upsampled_image = F.interpolate(downsampled_image, size=(height, width), mode='nearest')\n",
        "        result = upsampled_image * mask + x * (1. - mask)\n",
        "    else:\n",
        "        raise NotImplementedError('Not implemented mask type.')\n",
        "\n",
        "    return result, mask\n",
        "\n",
        "\n",
        "def spatial_discounting_mask(config):\n",
        "    \"\"\"Generate spatial discounting mask constant.\n",
        "\n",
        "    Spatial discounting mask is first introduced in publication:\n",
        "        Generative Image Inpainting with Contextual Attention, Yu et al.\n",
        "\n",
        "    Args:\n",
        "        config: Config should have configuration including HEIGHT, WIDTH,\n",
        "            DISCOUNTED_MASK.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: spatial discounting mask\n",
        "\n",
        "    \"\"\"\n",
        "    gamma = config['spatial_discounting_gamma']\n",
        "    height, width = config['mask_shape']\n",
        "    shape = [1, 1, height, width]\n",
        "    if config['discounted_mask']:\n",
        "        mask_values = np.ones((height, width))\n",
        "        for i in range(height):\n",
        "            for j in range(width):\n",
        "                mask_values[i, j] = max(\n",
        "                    gamma ** min(i, height - i),\n",
        "                    gamma ** min(j, width - j))\n",
        "        mask_values = np.expand_dims(mask_values, 0)\n",
        "        mask_values = np.expand_dims(mask_values, 0)\n",
        "    else:\n",
        "        mask_values = np.ones(shape)\n",
        "    spatial_discounting_mask_tensor = torch.tensor(mask_values, dtype=torch.float32)\n",
        "    if config['cuda']:\n",
        "        spatial_discounting_mask_tensor = spatial_discounting_mask_tensor.cuda()\n",
        "    return spatial_discounting_mask_tensor\n",
        "\n",
        "\n",
        "def reduce_mean(x, axis=None, keepdim=False):\n",
        "    if not axis:\n",
        "        axis = range(len(x.shape))\n",
        "    for i in sorted(axis, reverse=True):\n",
        "        x = torch.mean(x, dim=i, keepdim=keepdim)\n",
        "    return x\n",
        "\n",
        "\n",
        "def reduce_std(x, axis=None, keepdim=False):\n",
        "    if not axis:\n",
        "        axis = range(len(x.shape))\n",
        "    for i in sorted(axis, reverse=True):\n",
        "        x = torch.std(x, dim=i, keepdim=keepdim)\n",
        "    return x\n",
        "\n",
        "\n",
        "def reduce_sum(x, axis=None, keepdim=False):\n",
        "    if not axis:\n",
        "        axis = range(len(x.shape))\n",
        "    for i in sorted(axis, reverse=True):\n",
        "        x = torch.sum(x, dim=i, keepdim=keepdim)\n",
        "    return x\n",
        "\n",
        "\n",
        "def flow_to_image(flow):\n",
        "    \"\"\"Transfer flow map to image.\n",
        "    Part of code forked from flownet.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    maxu = -999.\n",
        "    maxv = -999.\n",
        "    minu = 999.\n",
        "    minv = 999.\n",
        "    maxrad = -1\n",
        "    for i in range(flow.shape[0]):\n",
        "        u = flow[i, :, :, 0]\n",
        "        v = flow[i, :, :, 1]\n",
        "        idxunknow = (abs(u) > 1e7) | (abs(v) > 1e7)\n",
        "        u[idxunknow] = 0\n",
        "        v[idxunknow] = 0\n",
        "        maxu = max(maxu, np.max(u))\n",
        "        minu = min(minu, np.min(u))\n",
        "        maxv = max(maxv, np.max(v))\n",
        "        minv = min(minv, np.min(v))\n",
        "        rad = np.sqrt(u ** 2 + v ** 2)\n",
        "        maxrad = max(maxrad, np.max(rad))\n",
        "        u = u / (maxrad + np.finfo(float).eps)\n",
        "        v = v / (maxrad + np.finfo(float).eps)\n",
        "        img = compute_color(u, v)\n",
        "        out.append(img)\n",
        "    return np.float32(np.uint8(out))\n",
        "\n",
        "\n",
        "def pt_flow_to_image(flow):\n",
        "    \"\"\"Transfer flow map to image.\n",
        "    Part of code forked from flownet.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    maxu = torch.tensor(-999)\n",
        "    maxv = torch.tensor(-999)\n",
        "    minu = torch.tensor(999)\n",
        "    minv = torch.tensor(999)\n",
        "    maxrad = torch.tensor(-1)\n",
        "    if torch.cuda.is_available():\n",
        "        maxu = maxu.cuda()\n",
        "        maxv = maxv.cuda()\n",
        "        minu = minu.cuda()\n",
        "        minv = minv.cuda()\n",
        "        maxrad = maxrad.cuda()\n",
        "    for i in range(flow.shape[0]):\n",
        "        u = flow[i, 0, :, :]\n",
        "        v = flow[i, 1, :, :]\n",
        "        idxunknow = (torch.abs(u) > 1e7) + (torch.abs(v) > 1e7)\n",
        "        u[idxunknow] = 0\n",
        "        v[idxunknow] = 0\n",
        "        maxu = torch.max(maxu, torch.max(u))\n",
        "        minu = torch.min(minu, torch.min(u))\n",
        "        maxv = torch.max(maxv, torch.max(v))\n",
        "        minv = torch.min(minv, torch.min(v))\n",
        "        rad = torch.sqrt((u ** 2 + v ** 2).float()).to(torch.int64)\n",
        "        maxrad = torch.max(maxrad, torch.max(rad))\n",
        "        u = u / (maxrad + torch.finfo(torch.float32).eps)\n",
        "        v = v / (maxrad + torch.finfo(torch.float32).eps)\n",
        "        # TODO: change the following to pytorch\n",
        "        img = pt_compute_color(u, v)\n",
        "        out.append(img)\n",
        "\n",
        "    return torch.stack(out, dim=0)\n",
        "\n",
        "\n",
        "def highlight_flow(flow):\n",
        "    \"\"\"Convert flow into middlebury color code image.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    s = flow.shape\n",
        "    for i in range(flow.shape[0]):\n",
        "        img = np.ones((s[1], s[2], 3)) * 144.\n",
        "        u = flow[i, :, :, 0]\n",
        "        v = flow[i, :, :, 1]\n",
        "        for h in range(s[1]):\n",
        "            for w in range(s[1]):\n",
        "                ui = u[h, w]\n",
        "                vi = v[h, w]\n",
        "                img[ui, vi, :] = 255.\n",
        "        out.append(img)\n",
        "    return np.float32(np.uint8(out))\n",
        "\n",
        "\n",
        "def pt_highlight_flow(flow):\n",
        "    \"\"\"Convert flow into middlebury color code image.\n",
        "        \"\"\"\n",
        "    out = []\n",
        "    s = flow.shape\n",
        "    for i in range(flow.shape[0]):\n",
        "        img = np.ones((s[1], s[2], 3)) * 144.\n",
        "        u = flow[i, :, :, 0]\n",
        "        v = flow[i, :, :, 1]\n",
        "        for h in range(s[1]):\n",
        "            for w in range(s[1]):\n",
        "                ui = u[h, w]\n",
        "                vi = v[h, w]\n",
        "                img[ui, vi, :] = 255.\n",
        "        out.append(img)\n",
        "    return np.float32(np.uint8(out))\n",
        "\n",
        "\n",
        "def compute_color(u, v):\n",
        "    h, w = u.shape\n",
        "    img = np.zeros([h, w, 3])\n",
        "    nanIdx = np.isnan(u) | np.isnan(v)\n",
        "    u[nanIdx] = 0\n",
        "    v[nanIdx] = 0\n",
        "    # colorwheel = COLORWHEEL\n",
        "    colorwheel = make_color_wheel()\n",
        "    ncols = np.size(colorwheel, 0)\n",
        "    rad = np.sqrt(u ** 2 + v ** 2)\n",
        "    a = np.arctan2(-v, -u) / np.pi\n",
        "    fk = (a + 1) / 2 * (ncols - 1) + 1\n",
        "    k0 = np.floor(fk).astype(int)\n",
        "    k1 = k0 + 1\n",
        "    k1[k1 == ncols + 1] = 1\n",
        "    f = fk - k0\n",
        "    for i in range(np.size(colorwheel, 1)):\n",
        "        tmp = colorwheel[:, i]\n",
        "        col0 = tmp[k0 - 1] / 255\n",
        "        col1 = tmp[k1 - 1] / 255\n",
        "        col = (1 - f) * col0 + f * col1\n",
        "        idx = rad <= 1\n",
        "        col[idx] = 1 - rad[idx] * (1 - col[idx])\n",
        "        notidx = np.logical_not(idx)\n",
        "        col[notidx] *= 0.75\n",
        "        img[:, :, i] = np.uint8(np.floor(255 * col * (1 - nanIdx)))\n",
        "    return img\n",
        "\n",
        "\n",
        "def pt_compute_color(u, v):\n",
        "    h, w = u.shape\n",
        "    img = torch.zeros([3, h, w])\n",
        "    if torch.cuda.is_available():\n",
        "        img = img.cuda()\n",
        "    nanIdx = (torch.isnan(u) + torch.isnan(v)) != 0\n",
        "    u[nanIdx] = 0.\n",
        "    v[nanIdx] = 0.\n",
        "    # colorwheel = COLORWHEEL\n",
        "    colorwheel = pt_make_color_wheel()\n",
        "    if torch.cuda.is_available():\n",
        "        colorwheel = colorwheel.cuda()\n",
        "    ncols = colorwheel.size()[0]\n",
        "    rad = torch.sqrt((u ** 2 + v ** 2).to(torch.float32))\n",
        "    a = torch.atan2(-v.to(torch.float32), -u.to(torch.float32)) / np.pi\n",
        "    fk = (a + 1) / 2 * (ncols - 1) + 1\n",
        "    k0 = torch.floor(fk).to(torch.int64)\n",
        "    k1 = k0 + 1\n",
        "    k1[k1 == ncols + 1] = 1\n",
        "    f = fk - k0.to(torch.float32)\n",
        "    for i in range(colorwheel.size()[1]):\n",
        "        tmp = colorwheel[:, i]\n",
        "        col0 = tmp[k0 - 1]\n",
        "        col1 = tmp[k1 - 1]\n",
        "        col = (1 - f) * col0 + f * col1\n",
        "        idx = rad <= 1. / 255.\n",
        "        col[idx] = 1 - rad[idx] * (1 - col[idx])\n",
        "        notidx = (idx != 0)\n",
        "        col[notidx] *= 0.75\n",
        "        img[i, :, :] = col * (1 - nanIdx).to(torch.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def make_color_wheel():\n",
        "    RY, YG, GC, CB, BM, MR = (15, 6, 4, 11, 13, 6)\n",
        "    ncols = RY + YG + GC + CB + BM + MR\n",
        "    colorwheel = np.zeros([ncols, 3])\n",
        "    col = 0\n",
        "    # RY\n",
        "    colorwheel[0:RY, 0] = 255\n",
        "    colorwheel[0:RY, 1] = np.transpose(np.floor(255 * np.arange(0, RY) / RY))\n",
        "    col += RY\n",
        "    # YG\n",
        "    colorwheel[col:col + YG, 0] = 255 - np.transpose(np.floor(255 * np.arange(0, YG) / YG))\n",
        "    colorwheel[col:col + YG, 1] = 255\n",
        "    col += YG\n",
        "    # GC\n",
        "    colorwheel[col:col + GC, 1] = 255\n",
        "    colorwheel[col:col + GC, 2] = np.transpose(np.floor(255 * np.arange(0, GC) / GC))\n",
        "    col += GC\n",
        "    # CB\n",
        "    colorwheel[col:col + CB, 1] = 255 - np.transpose(np.floor(255 * np.arange(0, CB) / CB))\n",
        "    colorwheel[col:col + CB, 2] = 255\n",
        "    col += CB\n",
        "    # BM\n",
        "    colorwheel[col:col + BM, 2] = 255\n",
        "    colorwheel[col:col + BM, 0] = np.transpose(np.floor(255 * np.arange(0, BM) / BM))\n",
        "    col += + BM\n",
        "    # MR\n",
        "    colorwheel[col:col + MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n",
        "    colorwheel[col:col + MR, 0] = 255\n",
        "    return colorwheel\n",
        "\n",
        "\n",
        "def pt_make_color_wheel():\n",
        "    RY, YG, GC, CB, BM, MR = (15, 6, 4, 11, 13, 6)\n",
        "    ncols = RY + YG + GC + CB + BM + MR\n",
        "    colorwheel = torch.zeros([ncols, 3])\n",
        "    col = 0\n",
        "    # RY\n",
        "    colorwheel[0:RY, 0] = 1.\n",
        "    colorwheel[0:RY, 1] = torch.arange(0, RY, dtype=torch.float32) / RY\n",
        "    col += RY\n",
        "    # YG\n",
        "    colorwheel[col:col + YG, 0] = 1. - (torch.arange(0, YG, dtype=torch.float32) / YG)\n",
        "    colorwheel[col:col + YG, 1] = 1.\n",
        "    col += YG\n",
        "    # GC\n",
        "    colorwheel[col:col + GC, 1] = 1.\n",
        "    colorwheel[col:col + GC, 2] = torch.arange(0, GC, dtype=torch.float32) / GC\n",
        "    col += GC\n",
        "    # CB\n",
        "    colorwheel[col:col + CB, 1] = 1. - (torch.arange(0, CB, dtype=torch.float32) / CB)\n",
        "    colorwheel[col:col + CB, 2] = 1.\n",
        "    col += CB\n",
        "    # BM\n",
        "    colorwheel[col:col + BM, 2] = 1.\n",
        "    colorwheel[col:col + BM, 0] = torch.arange(0, BM, dtype=torch.float32) / BM\n",
        "    col += BM\n",
        "    # MR\n",
        "    colorwheel[col:col + MR, 2] = 1. - (torch.arange(0, MR, dtype=torch.float32) / MR)\n",
        "    colorwheel[col:col + MR, 0] = 1.\n",
        "    return colorwheel\n",
        "\n",
        "\n",
        "def is_image_file(filename):\n",
        "    IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n",
        "    filename_lower = filename.lower()\n",
        "    return any(filename_lower.endswith(extension) for extension in IMG_EXTENSIONS)\n",
        "\n",
        "\n",
        "def deprocess(img):\n",
        "    img = img.add_(1).div_(2)\n",
        "    return img\n",
        "\n",
        "\n",
        "# get configs\n",
        "def get_config(config):\n",
        "    with open(config, 'r') as stream:\n",
        "        return yaml.load(stream)\n",
        "\n",
        "\n",
        "# Get model list for resume\n",
        "def get_model_list(dirname, key, iteration=0):\n",
        "    if os.path.exists(dirname) is False:\n",
        "        return None\n",
        "    gen_models = [os.path.join(dirname, f) for f in os.listdir(dirname) if\n",
        "                  os.path.isfile(os.path.join(dirname, f)) and key in f and \".pt\" in f]\n",
        "    if gen_models is None:\n",
        "        return None\n",
        "    gen_models.sort()\n",
        "    if iteration == 0:\n",
        "        last_model_name = gen_models[-1]\n",
        "    else:\n",
        "        for model_name in gen_models:\n",
        "            if '{:0>8d}'.format(iteration) in model_name:\n",
        "                return model_name\n",
        "        raise ValueError('Not found models with this iteration')\n",
        "    return last_model_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru8TmhFz6kBG"
      },
      "outputs": [],
      "source": [
        "def gen_conv(input_dim, output_dim, kernel_size=3, stride=1, padding=0, rate=1,\n",
        "             activation='elu'):\n",
        "    return Conv2dBlock(input_dim, output_dim, kernel_size, stride,\n",
        "                       conv_padding=padding, dilation=rate,\n",
        "                       activation=activation)\n",
        "\n",
        "\n",
        "def dis_conv(input_dim, output_dim, kernel_size=5, stride=2, padding=0, rate=1,\n",
        "             activation='lrelu'):\n",
        "    return Conv2dBlock(input_dim, output_dim, kernel_size, stride,\n",
        "                       conv_padding=padding, dilation=rate,\n",
        "                       activation=activation)\n",
        "\n",
        "\n",
        "class Conv2dBlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size, stride, padding=0,\n",
        "                 conv_padding=0, dilation=1, weight_norm='none', norm='none',\n",
        "                 activation='relu', pad_type='zero', transpose=False):\n",
        "        super(Conv2dBlock, self).__init__()\n",
        "        self.use_bias = True\n",
        "        # initialize padding\n",
        "        if pad_type == 'reflect':\n",
        "            self.pad = nn.ReflectionPad2d(padding)\n",
        "        elif pad_type == 'replicate':\n",
        "            self.pad = nn.ReplicationPad2d(padding)\n",
        "        elif pad_type == 'zero':\n",
        "            self.pad = nn.ZeroPad2d(padding)\n",
        "        elif pad_type == 'none':\n",
        "            self.pad = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
        "\n",
        "        # initialize normalization\n",
        "        norm_dim = output_dim\n",
        "        if norm == 'bn':\n",
        "            self.norm = nn.BatchNorm2d(norm_dim)\n",
        "        elif norm == 'in':\n",
        "            self.norm = nn.InstanceNorm2d(norm_dim)\n",
        "        elif norm == 'none':\n",
        "            self.norm = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported normalization: {}\".format(norm)\n",
        "\n",
        "        if weight_norm == 'sn':\n",
        "            self.weight_norm = spectral_norm_fn\n",
        "        elif weight_norm == 'wn':\n",
        "            self.weight_norm = weight_norm_fn\n",
        "        elif weight_norm == 'none':\n",
        "            self.weight_norm = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported normalization: {}\".format(weight_norm)\n",
        "\n",
        "        # initialize activation\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU(inplace=True)\n",
        "        elif activation == 'elu':\n",
        "            self.activation = nn.ELU(inplace=True)\n",
        "        elif activation == 'lrelu':\n",
        "            self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
        "        elif activation == 'prelu':\n",
        "            self.activation = nn.PReLU()\n",
        "        elif activation == 'selu':\n",
        "            self.activation = nn.SELU(inplace=True)\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'none':\n",
        "            self.activation = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
        "\n",
        "        # initialize convolution\n",
        "        if transpose:\n",
        "            self.conv = nn.ConvTranspose2d(input_dim, output_dim,\n",
        "                                           kernel_size, stride,\n",
        "                                           padding=conv_padding,\n",
        "                                           output_padding=conv_padding,\n",
        "                                           dilation=dilation,\n",
        "                                           bias=self.use_bias)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride,\n",
        "                                  padding=conv_padding, dilation=dilation,\n",
        "                                  bias=self.use_bias)\n",
        "\n",
        "        if self.weight_norm:\n",
        "            self.conv = self.weight_norm(self.conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.pad:\n",
        "            x = self.conv(self.pad(x))\n",
        "        else:\n",
        "            x = self.conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdK6yCgh5dk1"
      },
      "source": [
        "## CONTEXTUAL ATTENTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7pwgBt-lI2N",
        "outputId": "5ebbe72b-9aa9-4a0e-8472-ff1ce3d8758b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of imageA: torch.Size([2, 3, 256, 256])\n",
            "Size of imageB: torch.Size([2, 1, 256, 256])\n",
            "Generator\n",
            "CoarseGenerator\n",
            "FineGenerator\n",
            "ContextualAttention\n"
          ]
        }
      ],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.input_dim = 3\n",
        "        self.cnum = 32\n",
        "\n",
        "        self.coarse_generator = CoarseGenerator(self.input_dim, self.cnum)\n",
        "        self.fine_generator = FineGenerator(self.input_dim, self.cnum)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        print('Generator')\n",
        "        x_stage1 = self.coarse_generator(x, mask)\n",
        "        x_stage2, offset_flow = self.fine_generator(x, x_stage1, mask)\n",
        "        return x_stage1, x_stage2, offset_flow\n",
        "\n",
        "\n",
        "class CoarseGenerator(nn.Module):\n",
        "    def __init__(self, input_dim, cnum, use_cuda=True, device_ids=None):\n",
        "        super(CoarseGenerator, self).__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.device_ids = device_ids\n",
        "\n",
        "        self.conv1 = gen_conv(input_dim + 2, cnum, 5, 1, 2)\n",
        "        self.conv2_downsample = gen_conv(cnum, cnum*2, 3, 2, 1)\n",
        "        self.conv3 = gen_conv(cnum*2, cnum*2, 3, 1, 1)\n",
        "        self.conv4_downsample = gen_conv(cnum*2, cnum*4, 3, 2, 1)\n",
        "        self.conv5 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.conv6 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "\n",
        "        self.conv7_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 2, rate=2)\n",
        "        self.conv8_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 4, rate=4)\n",
        "        self.conv9_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 8, rate=8)\n",
        "        self.conv10_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 16, rate=16)\n",
        "\n",
        "        self.conv11 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.conv12 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "\n",
        "        self.conv13 = gen_conv(cnum*4, cnum*2, 3, 1, 1)\n",
        "        self.conv14 = gen_conv(cnum*2, cnum*2, 3, 1, 1)\n",
        "        self.conv15 = gen_conv(cnum*2, cnum, 3, 1, 1)\n",
        "        self.conv16 = gen_conv(cnum, cnum//2, 3, 1, 1)\n",
        "        self.conv17 = gen_conv(cnum//2, input_dim, 3, 1, 1, activation='none')\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # For indicating the boundaries of images\n",
        "        print('CoarseGenerator')\n",
        "        ones = torch.ones(x.size(0), 1, x.size(2), x.size(3))\n",
        "        if self.use_cuda:\n",
        "            ones = ones.cuda()\n",
        "            mask = mask.cuda()\n",
        "        # 5 x 256 x 256\n",
        "        x = self.conv1(torch.cat([x, ones, mask], dim=1))\n",
        "        x = self.conv2_downsample(x)\n",
        "        # cnum*2 x 128 x 128\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4_downsample(x)\n",
        "        # cnum*4 x 64 x 64\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7_atrous(x)\n",
        "        x = self.conv8_atrous(x)\n",
        "        x = self.conv9_atrous(x)\n",
        "        x = self.conv10_atrous(x)\n",
        "        x = self.conv11(x)\n",
        "        x = self.conv12(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        # cnum*2 x 128 x 128\n",
        "        x = self.conv13(x)\n",
        "        x = self.conv14(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        # cnum x 256 x 256\n",
        "        x = self.conv15(x)\n",
        "        x = self.conv16(x)\n",
        "        x = self.conv17(x)\n",
        "        # 3 x 256 x 256\n",
        "        x_stage1 = torch.clamp(x, -1., 1.)\n",
        "\n",
        "        return x_stage1\n",
        "\n",
        "\n",
        "class FineGenerator(nn.Module):\n",
        "    def __init__(self, input_dim, cnum, use_cuda=True, device_ids=None):\n",
        "        super(FineGenerator, self).__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.device_ids = device_ids\n",
        "\n",
        "        # 3 x 256 x 256\n",
        "        self.conv1 = gen_conv(input_dim + 2, cnum, 5, 1, 2)\n",
        "        self.conv2_downsample = gen_conv(cnum, cnum, 3, 2, 1)\n",
        "        # cnum*2 x 128 x 128\n",
        "        self.conv3 = gen_conv(cnum, cnum*2, 3, 1, 1)\n",
        "        self.conv4_downsample = gen_conv(cnum*2, cnum*2, 3, 2, 1)\n",
        "        # cnum*4 x 64 x 64\n",
        "        self.conv5 = gen_conv(cnum*2, cnum*4, 3, 1, 1)\n",
        "        self.conv6 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "\n",
        "        self.conv7_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 2, rate=2)\n",
        "        self.conv8_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 4, rate=4)\n",
        "        self.conv9_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 8, rate=8)\n",
        "        self.conv10_atrous = gen_conv(cnum*4, cnum*4, 3, 1, 16, rate=16)\n",
        "\n",
        "        # attention branch\n",
        "        # 3 x 256 x 256\n",
        "        self.pmconv1 = gen_conv(input_dim + 2, cnum, 5, 1, 2)\n",
        "        self.pmconv2_downsample = gen_conv(cnum, cnum, 3, 2, 1)\n",
        "        # cnum*2 x 128 x 128\n",
        "        self.pmconv3 = gen_conv(cnum, cnum*2, 3, 1, 1)\n",
        "        self.pmconv4_downsample = gen_conv(cnum*2, cnum*4, 3, 2, 1)\n",
        "        # cnum*4 x 64 x 64\n",
        "        self.pmconv5 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.pmconv6 = gen_conv(cnum*4, cnum*4, 3, 1, 1, activation='relu')\n",
        "        self.contextul_attention = ContextualAttention(ksize=3, stride=1, rate=2, fuse_k=3, softmax_scale=10,\n",
        "                                                       fuse=True, use_cuda=self.use_cuda, device_ids=self.device_ids)\n",
        "        self.pmconv9 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.pmconv10 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.allconv11 = gen_conv(cnum*8, cnum*4, 3, 1, 1)\n",
        "        self.allconv12 = gen_conv(cnum*4, cnum*4, 3, 1, 1)\n",
        "        self.allconv13 = gen_conv(cnum*4, cnum*2, 3, 1, 1)\n",
        "        self.allconv14 = gen_conv(cnum*2, cnum*2, 3, 1, 1)\n",
        "        self.allconv15 = gen_conv(cnum*2, cnum, 3, 1, 1)\n",
        "        self.allconv16 = gen_conv(cnum, cnum//2, 3, 1, 1)\n",
        "        self.allconv17 = gen_conv(cnum//2, input_dim, 3, 1, 1, activation='none')\n",
        "\n",
        "    def forward(self, xin, x_stage1, mask):\n",
        "        print('FineGenerator')\n",
        "        x1_inpaint = x_stage1 * mask + xin * (1. - mask)\n",
        "        # For indicating the boundaries of images\n",
        "        ones = torch.ones(xin.size(0), 1, xin.size(2), xin.size(3))\n",
        "        if self.use_cuda:\n",
        "            ones = ones.cuda()\n",
        "            mask = mask.cuda()\n",
        "        # conv branch\n",
        "        xnow = torch.cat([x1_inpaint, ones, mask], dim=1)\n",
        "        x = self.conv1(xnow)\n",
        "        x = self.conv2_downsample(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4_downsample(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7_atrous(x)\n",
        "        x = self.conv8_atrous(x)\n",
        "        x = self.conv9_atrous(x)\n",
        "        x = self.conv10_atrous(x)\n",
        "        x_hallu = x\n",
        "        # attention branch\n",
        "        x = self.pmconv1(xnow)\n",
        "        x = self.pmconv2_downsample(x)\n",
        "        x = self.pmconv3(x)\n",
        "        x = self.pmconv4_downsample(x)\n",
        "        x = self.pmconv5(x)\n",
        "        x = self.pmconv6(x)\n",
        "        x, offset_flow = self.contextul_attention(x, x, mask)\n",
        "        x = self.pmconv9(x)\n",
        "        x = self.pmconv10(x)\n",
        "        pm = x\n",
        "        x = torch.cat([x_hallu, pm], dim=1)\n",
        "        # merge two branches\n",
        "        x = self.allconv11(x)\n",
        "        x = self.allconv12(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = self.allconv13(x)\n",
        "        x = self.allconv14(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = self.allconv15(x)\n",
        "        x = self.allconv16(x)\n",
        "        x = self.allconv17(x)\n",
        "        x_stage2 = torch.clamp(x, -1., 1.)\n",
        "\n",
        "        return x_stage2, offset_flow\n",
        "\n",
        "\n",
        "class ContextualAttention(nn.Module):\n",
        "    def __init__(self, ksize=3, stride=1, rate=1, fuse_k=3, softmax_scale=10,\n",
        "                 fuse=False, use_cuda=False, device_ids=None):\n",
        "        super(ContextualAttention, self).__init__()\n",
        "        self.ksize = ksize\n",
        "        self.stride = stride\n",
        "        self.rate = rate\n",
        "        self.fuse_k = fuse_k\n",
        "        self.softmax_scale = softmax_scale\n",
        "        self.fuse = fuse\n",
        "        self.use_cuda = use_cuda\n",
        "        self.device_ids = device_ids\n",
        "\n",
        "    def forward(self, f, b, mask=None):\n",
        "        \"\"\" Contextual attention layer implementation.\n",
        "        Contextual attention is first introduced in publication:\n",
        "            Generative Image Inpainting with Contextual Attention, Yu et al.\n",
        "        Args:\n",
        "            f: Input feature to match (foreground).\n",
        "            b: Input feature for match (background).\n",
        "            mask: Input mask for b, indicating patches not available.\n",
        "            ksize: Kernel size for contextual attention.\n",
        "            stride: Stride for extracting patches from b.\n",
        "            rate: Dilation for matching.\n",
        "            softmax_scale: Scaled softmax for attention.\n",
        "        Returns:\n",
        "            torch.tensor: output\n",
        "        \"\"\"\n",
        "        print('ContextualAttention')\n",
        "        # get shapes\n",
        "        raw_int_fs = list(f.size())   # b*c*h*w\n",
        "        raw_int_bs = list(b.size())   # b*c*h*w\n",
        "        # extract patches from background with stride and rate\n",
        "        kernel = 2 * self.rate\n",
        "        # raw_w is extracted for reconstruction\n",
        "        raw_w = extract_image_patches(b, ksizes=[kernel, kernel],\n",
        "                                      strides=[self.rate*self.stride,\n",
        "                                               self.rate*self.stride],\n",
        "                                      rates=[1, 1],\n",
        "                                      padding='same') # [N, C*k*k, L]\n",
        "        # raw_shape: [N, C, k, k, L]\n",
        "        raw_w = raw_w.view(raw_int_bs[0], raw_int_bs[1], kernel, kernel, -1)\n",
        "        raw_w = raw_w.permute(0, 4, 1, 2, 3)    # raw_shape: [N, L, C, k, k]\n",
        "        raw_w_groups = torch.split(raw_w, 1, dim=0)\n",
        "        # downscaling foreground option: downscaling both foreground and\n",
        "        # background for matching and use original background for reconstruction.\n",
        "        f = F.interpolate(f, scale_factor=1./self.rate, mode='nearest')\n",
        "        b = F.interpolate(b, scale_factor=1./self.rate, mode='nearest')\n",
        "        int_fs = list(f.size())     # b*c*h*w\n",
        "        int_bs = list(b.size())\n",
        "        f_groups = torch.split(f, 1, dim=0)  # split tensors along the batch dimension\n",
        "        # w shape: [N, C*k*k, L]\n",
        "        w = extract_image_patches(b, ksizes=[self.ksize, self.ksize],\n",
        "                                  strides=[self.stride, self.stride],\n",
        "                                  rates=[1, 1],\n",
        "                                  padding='same')\n",
        "        # w shape: [N, C, k, k, L]\n",
        "        w = w.view(int_bs[0], int_bs[1], self.ksize, self.ksize, -1)\n",
        "        w = w.permute(0, 4, 1, 2, 3)    # w shape: [N, L, C, k, k]\n",
        "        w_groups = torch.split(w, 1, dim=0)\n",
        "        # process mask\n",
        "        if mask is None:\n",
        "            mask = torch.zeros([int_bs[0], 1, int_bs[2], int_bs[3]])\n",
        "            if self.use_cuda:\n",
        "                mask = mask.cuda()\n",
        "        else:\n",
        "            mask = F.interpolate(mask, scale_factor=1./(4*self.rate), mode='nearest')\n",
        "        int_ms = list(mask.size())\n",
        "        # m shape: [N, C*k*k, L]\n",
        "        m = extract_image_patches(mask, ksizes=[self.ksize, self.ksize],\n",
        "                                  strides=[self.stride, self.stride],\n",
        "                                  rates=[1, 1],\n",
        "                                  padding='same')\n",
        "        # m shape: [N, C, k, k, L]\n",
        "        m = m.view(int_ms[0], int_ms[1], self.ksize, self.ksize, -1)\n",
        "        m = m.permute(0, 4, 1, 2, 3)    # m shape: [N, L, C, k, k]\n",
        "        m = m[0]    # m shape: [L, C, k, k]\n",
        "        # mm shape: [L, 1, 1, 1]\n",
        "        mm = (reduce_mean(m, axis=[1, 2, 3], keepdim=True)==0.).to(torch.float32)\n",
        "        mm = mm.permute(1, 0, 2, 3) # mm shape: [1, L, 1, 1]\n",
        "\n",
        "        y = []\n",
        "        offsets = []\n",
        "        k = self.fuse_k\n",
        "        scale = self.softmax_scale    # to fit the PyTorch tensor image value range\n",
        "        fuse_weight = torch.eye(k).view(1, 1, k, k)  # 1*1*k*k\n",
        "        if self.use_cuda:\n",
        "            fuse_weight = fuse_weight.cuda()\n",
        "\n",
        "        for xi, wi, raw_wi in zip(f_groups, w_groups, raw_w_groups):\n",
        "            '''\n",
        "            O => output channel as a conv filter\n",
        "            I => input channel as a conv filter\n",
        "            xi : separated tensor along batch dimension of front; (B=1, C=128, H=32, W=32)\n",
        "            wi : separated patch tensor along batch dimension of back; (B=1, O=32*32, I=128, KH=3, KW=3)\n",
        "            raw_wi : separated tensor along batch dimension of back; (B=1, I=32*32, O=128, KH=4, KW=4)\n",
        "            '''\n",
        "            # conv for compare\n",
        "            escape_NaN = torch.FloatTensor([1e-4])\n",
        "            if self.use_cuda:\n",
        "                escape_NaN = escape_NaN.cuda()\n",
        "            wi = wi[0]  # [L, C, k, k]\n",
        "            max_wi = torch.sqrt(reduce_sum(torch.pow(wi, 2) + escape_NaN, axis=[1, 2, 3], keepdim=True))\n",
        "            wi_normed = wi / max_wi\n",
        "            # xi shape: [1, C, H, W], yi shape: [1, L, H, W]\n",
        "            xi = same_padding(xi, [self.ksize, self.ksize], [1, 1], [1, 1])  # xi: 1*c*H*W\n",
        "            yi = F.conv2d(xi, wi_normed, stride=1)   # [1, L, H, W]\n",
        "            # conv implementation for fuse scores to encourage large patches\n",
        "            if self.fuse:\n",
        "                # make all of depth to spatial resolution\n",
        "                yi = yi.view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])  # (B=1, I=1, H=32*32, W=32*32)\n",
        "                yi = same_padding(yi, [k, k], [1, 1], [1, 1])\n",
        "                yi = F.conv2d(yi, fuse_weight, stride=1)  # (B=1, C=1, H=32*32, W=32*32)\n",
        "                yi = yi.contiguous().view(1, int_bs[2], int_bs[3], int_fs[2], int_fs[3])  # (B=1, 32, 32, 32, 32)\n",
        "                yi = yi.permute(0, 2, 1, 4, 3)\n",
        "                yi = yi.contiguous().view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])\n",
        "                yi = same_padding(yi, [k, k], [1, 1], [1, 1])\n",
        "                yi = F.conv2d(yi, fuse_weight, stride=1)\n",
        "                yi = yi.contiguous().view(1, int_bs[3], int_bs[2], int_fs[3], int_fs[2])\n",
        "                yi = yi.permute(0, 2, 1, 4, 3).contiguous()\n",
        "            yi = yi.view(1, int_bs[2] * int_bs[3], int_fs[2], int_fs[3])  # (B=1, C=32*32, H=32, W=32)\n",
        "            # softmax to match\n",
        "            yi = yi * mm\n",
        "            yi = F.softmax(yi*scale, dim=1)\n",
        "            yi = yi * mm  # [1, L, H, W]\n",
        "\n",
        "            offset = torch.argmax(yi, dim=1, keepdim=True)  # 1*1*H*W\n",
        "\n",
        "            if int_bs != int_fs:\n",
        "                # Normalize the offset value to match foreground dimension\n",
        "                times = float(int_fs[2] * int_fs[3]) / float(int_bs[2] * int_bs[3])\n",
        "                offset = ((offset + 1).float() * times - 1).to(torch.int64)\n",
        "            offset = torch.cat([offset//int_fs[3], offset%int_fs[3]], dim=1)  # 1*2*H*W\n",
        "\n",
        "            # deconv for patch pasting\n",
        "            wi_center = raw_wi[0]\n",
        "            # yi = F.pad(yi, [0, 1, 0, 1])    # here may need conv_transpose same padding\n",
        "            yi = F.conv_transpose2d(yi, wi_center, stride=self.rate, padding=1) / 4.  # (B=1, C=128, H=64, W=64)\n",
        "            y.append(yi)\n",
        "            offsets.append(offset)\n",
        "\n",
        "        y = torch.cat(y, dim=0)  # back to the mini-batch\n",
        "        y.contiguous().view(raw_int_fs)\n",
        "\n",
        "        offsets = torch.cat(offsets, dim=0)\n",
        "        offsets = offsets.view(int_fs[0], 2, *int_fs[2:])\n",
        "\n",
        "        # case1: visualize optical flow: minus current position\n",
        "        h_add = torch.arange(int_fs[2]).view([1, 1, int_fs[2], 1]).expand(int_fs[0], -1, -1, int_fs[3])\n",
        "        w_add = torch.arange(int_fs[3]).view([1, 1, 1, int_fs[3]]).expand(int_fs[0], -1, int_fs[2], -1)\n",
        "        ref_coordinate = torch.cat([h_add, w_add], dim=1)\n",
        "        if self.use_cuda:\n",
        "            ref_coordinate = ref_coordinate.cuda()\n",
        "\n",
        "        offsets = offsets - ref_coordinate\n",
        "        # flow = pt_flow_to_image(offsets)\n",
        "\n",
        "        flow = torch.from_numpy(flow_to_image(offsets.permute(0, 2, 3, 1).cpu().data.numpy())) / 255.\n",
        "        flow = flow.permute(0, 3, 1, 2)\n",
        "        if self.use_cuda:\n",
        "            flow = flow.cuda()\n",
        "        # case2: visualize which pixels are attended\n",
        "        # flow = torch.from_numpy(highlight_flow((offsets * mask.long()).cpu().data.numpy()))\n",
        "\n",
        "        if self.rate != 1:\n",
        "            flow = F.interpolate(flow, scale_factor=self.rate*4, mode='nearest')\n",
        "\n",
        "        return y, flow\n",
        "\n",
        "class LocalDis(nn.Module):\n",
        "    def __init__(self, config, use_cuda=True, device_ids=None):\n",
        "        super(LocalDis, self).__init__()\n",
        "        self.input_dim = 3\n",
        "        self.cnum = 32\n",
        "        self.use_cuda = use_cuda\n",
        "        self.device_ids = device_ids\n",
        "\n",
        "        self.dis_conv_module = DisConvModule(self.input_dim, self.cnum)\n",
        "        self.linear = nn.Linear(self.cnum*4*8*8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('LocalDis')\n",
        "        x = self.dis_conv_module(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GlobalDis(nn.Module):\n",
        "    def __init__(self, config, use_cuda=True, device_ids=None):\n",
        "        super(GlobalDis, self).__init__()\n",
        "        self.input_dim = 3\n",
        "        self.cnum = 32\n",
        "        self.use_cuda = use_cuda\n",
        "        self.device_ids = device_ids\n",
        "\n",
        "        self.dis_conv_module = DisConvModule(self.input_dim, self.cnum)\n",
        "        self.linear = nn.Linear(self.cnum*4*16*16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('GlobalDis')\n",
        "        x = self.dis_conv_module(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DisConvModule(nn.Module):\n",
        "    def __init__(self, input_dim, cnum, use_cuda=True, device_ids=None):\n",
        "        super(DisConvModule, self).__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.device_ids = device_ids\n",
        "\n",
        "        self.conv1 = dis_conv(input_dim, cnum, 5, 2, 2)\n",
        "        self.conv2 = dis_conv(cnum, cnum*2, 5, 2, 2)\n",
        "        self.conv3 = dis_conv(cnum*2, cnum*4, 5, 2, 2)\n",
        "        self.conv4 = dis_conv(cnum*4, cnum*4, 5, 2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def test_contextual_attention():\n",
        "    import cv2\n",
        "    import os\n",
        "\n",
        "    rate = 2\n",
        "    stride = 1\n",
        "    grid = rate*stride\n",
        "    image = torch.randn((2, 3, 256, 256))\n",
        "    batch,dim,w, h = image.shape\n",
        "    # b = b.resize((w//grid*grid, h//grid*grid), Image.ANTIALIAS)\n",
        "    print('Size of imageA: {}'.format(image.shape))\n",
        "\n",
        "    mask = torch.randn((2, 1, 256, 256))\n",
        "    print('Size of imageB: {}'.format(mask.shape))\n",
        "    #f, image = mask.unsqueeze(0), image.unsqueeze(0)\n",
        "    if torch.cuda.is_available():\n",
        "        mask, image = mask.cuda(), image.cuda()\n",
        "\n",
        "    contextual_attention = Generator()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        contextual_attention = contextual_attention.cuda()\n",
        "\n",
        "    pred = contextual_attention(image, mask)\n",
        "\n",
        "test_contextual_attention()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOa3cCXSArTP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhfHtK5G4yuV"
      },
      "source": [
        "# UNET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_mQUFmT5StW"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=3, mid_channels=[64, 128,256,512],\n",
        "    ):\n",
        "        super(UNET, self).__init__()\n",
        "        self.double_conv = DoubleConv(3, 32)\n",
        "        self.double_conv_end = DoubleConv(32, 32)\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        temp_channel = 32\n",
        "        for channel in mid_channels:\n",
        "            self.downs.append(DoubleConv(temp_channel, channel))\n",
        "            temp_channel = channel\n",
        "\n",
        "        self.middle_conv = nn.Sequential(self.pool, DoubleConv(mid_channels[-1], mid_channels[-1] * 2))\n",
        "\n",
        "        for channel in reversed(mid_channels):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    channel * 2, channel, kernel_size=2, stride=2, ))\n",
        "            self.ups.append(DoubleConv(channel * 2, channel))\n",
        "\n",
        "        self.out_conv = nn.Sequential(nn.ConvTranspose2d(\n",
        "            mid_channels[0], 32, kernel_size=2, stride=2, ),\n",
        "            DoubleConv(32, 32),\n",
        "            nn.Conv2d(32, out_channels, kernel_size=1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        copied_data = []\n",
        "        x = self.double_conv(x)\n",
        "        copied_data.append(x)\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = self.pool(x)\n",
        "            x = down(x)\n",
        "            copied_data.append(x)\n",
        "\n",
        "        x = self.middle_conv(x)\n",
        "        copied_data = copied_data[::-1]\n",
        "\n",
        "        for i in range(len(self.ups)):\n",
        "\n",
        "            if i % 2 == 0:\n",
        "                x = self.ups[i](x)\n",
        "            else:\n",
        "                concat_data = copied_data[i // 2]\n",
        "                concat_skip = torch.cat((concat_data, x), dim=1)\n",
        "                x = self.ups[i](concat_skip)\n",
        "\n",
        "        x = self.out_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def test():\n",
        "    x = torch.randn((2, 3, 256, 256))\n",
        "    model = UNET(in_channels=3, out_channels=3)\n",
        "    preds = model(x)\n",
        "    print(x.shape)\n",
        "    print(preds.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xerybRz9DvH",
        "outputId": "22743c2c-80f2-48af-94d8-07c5cd514539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 256, 256])\n",
            "torch.Size([2, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCJuQORl5ysb"
      },
      "source": [
        "# TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkvmcW9SHxUo"
      },
      "outputs": [],
      "source": [
        "psnr = PeakSignalNoiseRatio().to(device=DEVICE)\n",
        "ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP9IdI1X56mv"
      },
      "outputs": [],
      "source": [
        "def train_fn(loader, model, optimizer, loss_fn, scaler=None):\n",
        "    loop = tqdm(loader)\n",
        "    sum_loss = 0\n",
        "    for batch_idx, (image, _, masked_image) in enumerate(loop):\n",
        "        masked_image = masked_image.to(device=DEVICE)\n",
        "        image = image.to(device=DEVICE)\n",
        "        #mask = mask.to(device=DEVICE)\n",
        "      \n",
        "        pred = model(masked_image)\n",
        "        loss = loss_fn(pred, image)\n",
        "\n",
        "        sum_loss += loss.item()\n",
        "        PSNR = psnr(pred,image).item()\n",
        "        SSIM = ssim(pred,image).item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        wandb.log({\"train_loss\": loss.item()})\n",
        "        loop.set_postfix(loss=loss.item(),PSNR=PSNR,SSIM=SSIM)\n",
        "\n",
        "        if batch_idx % (len(loader) // 5) == 0:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint)\n",
        "    wandb.log({\"train_loss_mean\": sum_loss/len(loop)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_TTrHo352jd"
      },
      "source": [
        "# MAIN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq2k5_rt6LnF"
      },
      "outputs": [],
      "source": [
        "model = UNET().to(DEVICE)\n",
        "\n",
        "wandb.watch(\n",
        "    model,\n",
        "    log= \"all\",\n",
        "    log_freq = 10,\n",
        "    log_graph = True)\n",
        "\n",
        "loss_fn = nn.BCELoss(reduction='sum')\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def main():\n",
        "    import sys, traceback, gc\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    if LOAD_MODEL:\n",
        "        # best_model = wandb.restore(\"UnetGen.pth.tar\",\"rkozlyak/WatermarkRemoval/5cb8er9z\")\n",
        "        # torch.load(best_model.name)\n",
        "        load_checkpoint(torch.load(\"UnetGen.pth.tar\"), model)\n",
        "    \n",
        "    for epoch in range(1,NUM_EPOCHS):\n",
        "        \n",
        "        train_fn(train_loader, model, optimizer, loss_fn)\n",
        "        \n",
        "        checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "        }\n",
        "        save_checkpoint(checkpoint)\n",
        "        check_accuracy(val_loader, model,loss_fn,log_images=(epoch==(NUM_EPOCHS-1)), device=DEVICE)\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3r3TRy875mO-",
        "48Yx0O2PLU2x",
        "J7-ciyV15tcn",
        "NIVZxHbKNFrs",
        "O-xlBAUW--jh",
        "0mdZ_Xr9-LGh",
        "3rfiMnsQ-X52",
        "WMObyBSw-iAJ",
        "Qrhmiovm-rNK",
        "lX2U4t_Jn7Kh",
        "4XDGjSc1k4r9",
        "FpifU30xnvnE",
        "T9W26j0jt3N5",
        "NsknvOLpTnXh",
        "dv2N1s0ljjdP",
        "0IrYaeC2sFpW",
        "2QieVKs-v67H",
        "DqFnAW-57h_f",
        "DHf1YyrH9vsF",
        "V28EHkvT5VKE",
        "HdK6yCgh5dk1"
      ],
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPyPiD5E9FW82wyTkW4SaOt",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "37fd76887887412783be78d0354ef0a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_959cceb58414455e81553e4b2b816e9f",
              "IPY_MODEL_3a164ead9f2f4118967d30551ff9e1c8"
            ],
            "layout": "IPY_MODEL_ca9d79ee83214ff492eb4d0b798f67b0"
          }
        },
        "959cceb58414455e81553e4b2b816e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35b51985e7414503ab195b1f6ead3728",
            "placeholder": "​",
            "style": "IPY_MODEL_52b4ec395bf94ac4a0a45f360b896d55",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "3a164ead9f2f4118967d30551ff9e1c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a45fcb46ca048cf8636b8795b6cc7b3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6474340aa15940b1b11d5efc67d2a914",
            "value": 1
          }
        },
        "ca9d79ee83214ff492eb4d0b798f67b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35b51985e7414503ab195b1f6ead3728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52b4ec395bf94ac4a0a45f360b896d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a45fcb46ca048cf8636b8795b6cc7b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6474340aa15940b1b11d5efc67d2a914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}